{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/snli_1.0/'\n",
    "data_frame = pd.read_csv(data_path + 'snli_1.0_train.txt', sep='\\t')\n",
    "gold_labels = data_frame.gold_label.tolist()\n",
    "sentences1 = data_frame.sentence1.tolist()\n",
    "sentences2 = data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10000 samples from the dataset\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10000\n",
    "samples = [(sentences1[i], sentences2[i], gold_labels[i]) for i in range(num_samples)]\n",
    "print(\"Using {} samples from the dataset\".format(len(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(num_layers, input_first_layer, filters):\n",
    "    inputs = []\n",
    "    inputs.append(input_first_layer)\n",
    "    for i in range(num_layers):\n",
    "        output_conv = Conv1D(nb_filter=filters, filter_length=3, activation='relu')(inputs[i])\n",
    "        inputs.append(output_conv)\n",
    "    output = MaxPooling1D()(inputs[i+1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size = (10,128)):\n",
    "    inputs = Input(shape=input_size)\n",
    "    outputs = conv_block(1, inputs, 64)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 10, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 8, 64)         24640       input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 4, 64)         0           convolution1d_5[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 24,640\n",
      "Trainable params: 24,640\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "#todo use word embeddings instead of one-hot\n",
    "max_length_sentence = np.max([len(sentence.split()) for sentence, sentence2, labels in samples])\n",
    "print(max_length_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, vectorspace=vectorspace):\n",
    "    result = np.array([])\n",
    "    missing_words = []\n",
    "    for word in sentence.split():  #TODO apply better tokenizing\n",
    "        try:\n",
    "            result = np.concatenate((result, vectorspace[word]))\n",
    "        except:\n",
    "            print(\"Missing {}\".format(word))\n",
    "            missing_words.append(word)\n",
    "    #print(np.shape(result))\n",
    "    #print(len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5843 unique tokens.\n",
      "Shape of data tensor: (2, 10000, 1000)\n",
      "Shape of label tensor: (10000, 4)\n",
      "Shape of x_train: (2, 8000, 1000)\n",
      "Shape of y_train: (8000, 4)\n",
      "Shape of x_val: (2, 2000, 1000)\n",
      "Shape of y_val: (2000, 4)\n",
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (5844, 300)\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [sentence for sentence, sentence2, label in samples]\n",
    "sentences2 = [sentence2 for sentence, sentence2, label in samples]\n",
    "labels = [label for sentence, sentence2, label in samples] \n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences1 + sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data1 = sequence.pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = sequence.pad_sequences(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "data = np.asarray([data1, data2])\n",
    "\n",
    "labels = to_categorical(np.asarray(numeric_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[1])\n",
    "np.random.shuffle(indices)\n",
    "data = data[:,indices,:]\n",
    "labels = labels[indices,:]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "x_train = data[:,:-num_validation_samples,:]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[:,-num_validation_samples:,:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo next:\n",
    "+ build model architecture that uses the same layers for encoding both sentences (starting with the tutorial used above)\n",
    "+ build model on top to predict entailment, neutral, contradiction (see intro functional, shared layers)\n",
    "+ test different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_layer(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24 samples, validate on 6 samples\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 0s - loss: 1.0977 - acc: 0.3750 - val_loss: 1.1004 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 0s - loss: 1.0949 - acc: 0.3750 - val_loss: 1.1054 - val_acc: 0.3333\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 0s - loss: 1.0886 - acc: 0.3750 - val_loss: 1.1180 - val_acc: 0.3333\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 0s - loss: 1.0765 - acc: 0.3750 - val_loss: 1.1489 - val_acc: 0.3333\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 0s - loss: 1.0572 - acc: 0.3750 - val_loss: 1.2161 - val_acc: 0.3333\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 0s - loss: 1.0337 - acc: 0.3750 - val_loss: 1.3344 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 0s - loss: 1.0116 - acc: 0.3750 - val_loss: 1.4287 - val_acc: 0.3333\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 0s - loss: 0.9850 - acc: 0.3750 - val_loss: 1.4522 - val_acc: 0.3333\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 0s - loss: 0.9500 - acc: 0.3750 - val_loss: 1.4293 - val_acc: 0.3333\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 0s - loss: 0.9120 - acc: 0.3750 - val_loss: 1.3873 - val_acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23438f3160>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train[0,:,:], x_train[1,:,:]], y_train, \n",
    "          nb_epoch=10, \n",
    "          validation_data=([x_val[0,:,:], x_val[1,:,:]], y_val),\n",
    "          batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??model.fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
