{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOCAL_MODEL_PATH = \"/home/fabian/work/ag_sc/repeval2017/models/\"\n",
    "HARD_DRIVE_MODEL_PATH = \"/media/fabian/MACWIN/deep_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT-LIST:\n",
    "\n",
    "max + avg pooling makes no real difference in Conv+Lstm\n",
    "- Conv, LSTM, Attention (rest like below)\n",
    "        - SNLI acc 0.73\n",
    "        - MNLI acc 0.64\n",
    "- Conv (128, kernelsize 5) + LSTM 64 + 521 Dense -> 64 Dense, 10 epochs, Adam -> \n",
    "        - SNLI acc 0.71 \n",
    "        - MNLI acc 0.62\n",
    "- Conv_pool, 3 modules, 10 epochs, ADAM\n",
    "        - SNLI acc 0.68\n",
    "        - MNLI acc 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "snli_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_train.txt', sep='\\t')\n",
    "snli_gold_labels = snli_data_frame.gold_label.tolist()\n",
    "snli_sentences1 = snli_data_frame.sentence1.tolist()\n",
    "snli_sentences2 = snli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnli_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_train.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_gold_labels = mnli_data_frame.gold_label.tolist()\n",
    "mnli_sentences1 = mnli_data_frame.sentence1.tolist()\n",
    "mnli_sentences2 = mnli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_num_samples= len(snli_gold_labels)\n",
    "snli_samples = [(str(snli_sentences1[i]), str(snli_sentences2[i]), str(snli_gold_labels[i])) for i in range(snli_num_samples)]\n",
    "print(\"Using {} training-samples from the snli-trainset\".format(snli_num_samples))\n",
    "mnli_num_samples= len(mnli_gold_labels)\n",
    "mnli_samples = [(str(mnli_sentences1[i]), str(mnli_sentences2[i]), str(mnli_gold_labels[i])) for i in range(mnli_num_samples)]\n",
    "print(\"Using {} training-samples from the multinli-trainset\".format(mnli_num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnli_test_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_dev_matched.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_test_gold_labels = mnli_test_data_frame.gold_label.tolist()\n",
    "mnli_test_sentences1 = mnli_test_data_frame.sentence1.tolist()\n",
    "mnli_test_sentences2 = mnli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_test_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_test.txt', sep='\\t')\n",
    "snli_test_gold_labels = snli_test_data_frame.gold_label.tolist()\n",
    "snli_test_sentences1 = snli_test_data_frame.sentence1.tolist()\n",
    "snli_test_sentences2 = snli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_test_num_samples= len(snli_test_gold_labels)\n",
    "snli_test_samples = [(str(snli_test_sentences1[i]), str(snli_test_sentences2[i]), str(snli_test_gold_labels[i])) for i in range(snli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the snli-testset\".format(snli_test_num_samples))\n",
    "mnli_test_num_samples= len(mnli_test_gold_labels)\n",
    "mnli_test_samples = [(str(mnli_test_sentences1[i]), str(mnli_test_sentences2[i]), str(mnli_test_gold_labels[i])) for i in range(mnli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the multinli-testset\".format(mnli_test_num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use h5py to store the data. This helps to use less RAM for the training data in training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 0.0 #there is some extra testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for padding without using numpy, which breaks when building data arrays with 500000 or more embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_padding(sequences, maxlen=100):\n",
    "    for sequence in sequences:\n",
    "        while len(sequence) < maxlen:\n",
    "            sequence.append(0)\n",
    "        if len(sequence) > maxlen:\n",
    "            del sequence[maxlen:]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn string sequences into integer sequences, pad them to equal length, divide into training and validation data, store using the hdf5 binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str_sentences1 = [sentence for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "str_sentences2 = [sentence2 for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "labels = [label for sentence, sentence2, label in snli_samples + mnli_samples] \n",
    "\n",
    "snli_test_str_sentences1 = [sentence for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_labels = [label for sentence, sentence2, label in snli_test_samples] \n",
    "\n",
    "mnli_test_str_sentences1 = [sentence for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_labels = [label for sentence, sentence2, label in mnli_test_samples] \n",
    "\n",
    "numeric_labels = []\n",
    "for label in labels + snli_test_labels + mnli_test_labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(str_sentences1 + str_sentences2 + \n",
    "                       snli_test_str_sentences1 + snli_test_str_sentences2 +\n",
    "                      mnli_test_str_sentences1 + mnli_test_str_sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(str_sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(str_sentences2)\n",
    "snli_test_sequences1 = tokenizer.texts_to_sequences(snli_test_str_sentences1)\n",
    "snli_test_sequences2 = tokenizer.texts_to_sequences(snli_test_str_sentences2)\n",
    "mnli_test_sequences1 = tokenizer.texts_to_sequences(mnli_test_str_sentences1)\n",
    "mnli_test_sequences2 = tokenizer.texts_to_sequences(mnli_test_str_sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Saving Tokenizer...\")\n",
    "pickle.dump(tokenizer, open(\"../data/tokenizer.p\", \"wb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Saving word index...\")\n",
    "pickle.dump(word_index, open(\"../data/word_index.p\", \"wb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading word index...\")\n",
    "word_index = pickle.load(open(\"../data/word_index.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of sequences1: {}.\".format(np.shape(sequences1)))\n",
    "print(\"shape of sequences2: {}.\".format(np.shape(sequences2)))\n",
    "print(\"shape of snli_test_sequences1: {}.\".format(np.shape(snli_test_sequences1)))\n",
    "print(\"shape of snli_test_sequences2: {}.\".format(np.shape(snli_test_sequences2)))\n",
    "print(\"mhape of snli_test_sequences1: {}.\".format(np.shape(mnli_test_sequences1)))\n",
    "print(\"mhape of snli_test_sequences2: {}.\".format(np.shape(mnli_test_sequences2)))\n",
    "\n",
    "data1 = simple_padding(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = simple_padding(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data1 = simple_padding(snli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data2 = simple_padding(snli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data1 = simple_padding(mnli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data2 = simple_padding(mnli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"shape of data1: {}.\".format(np.shape(data1)))\n",
    "print(\"shape of data2: {}.\".format(np.shape(data2)))\n",
    "print(\"shape of snli_test_data1: {}.\".format(np.shape(snli_test_data1)))\n",
    "print(\"shape of snli_test_data2: {}.\".format(np.shape(snli_test_data2)))\n",
    "print(\"shape of mnli_test_data1: {}.\".format(np.shape(mnli_test_data1)))\n",
    "print(\"shape of mnli_test_data2: {}.\".format(np.shape(mnli_test_data2)))\n",
    "\n",
    "training_data = np.asarray([data1, data2])\n",
    "snli_testing_data = np.asarray([snli_test_data1, snli_test_data2])\n",
    "mnli_testing_data = np.asarray([mnli_test_data1, mnli_test_data2])\n",
    "\n",
    "all_labels = to_categorical(np.asarray(numeric_labels))\n",
    "training_labels = all_labels[:len(labels)]\n",
    "snli_testing_labels = all_labels[len(labels):len(labels) + len(snli_test_labels)]\n",
    "mnli_testing_labels = all_labels[len(labels) + len(snli_test_labels):]\n",
    "\n",
    "##########CONTINUE HERE###############\n",
    "\n",
    "print('Shape of training_data tensor:', training_data.shape)\n",
    "print('Shape of training_labels tensor:', training_labels.shape)\n",
    "print('Shape of snli_testing_data tensor:', snli_testing_data.shape)\n",
    "print('Shape of snli_testing_labels tensor:', snli_testing_labels.shape)\n",
    "print('Shape of mnli_testing_data tensor:', mnli_testing_data.shape)\n",
    "print('Shape of mnli_testing_labels tensor:', mnli_testing_labels.shape)\n",
    "\n",
    "# not needed here, because training and testing data are already splitted\n",
    "# split the data into a training set and a validation set\n",
    "#indices = np.arange(data.shape[1])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[:,indices,:]\n",
    "#labels = labels[indices,:]\n",
    "#num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "#x_train = data[:,:-num_validation_samples,:]\n",
    "#y_train = labels[:-num_validation_samples]\n",
    "#x_val = data[:,-num_validation_samples:,:]\n",
    "#y_val = labels[-num_validation_samples:]\n",
    "\n",
    "#print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "#print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "#print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "#print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print(\"Storing training and test data to hdf5...\")\n",
    "\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "if not 'training_data' in f: x_train = f.create_dataset('training_data', data=training_data)\n",
    "if not 'training_labels' in f: y_train = f.create_dataset('training_labels', data=training_labels)\n",
    "if not 'snli_testing_data' in f: snli_x_val = f.create_dataset('snli_testing_data', data=snli_testing_data)\n",
    "if not 'snli_testing_labels' in f: snli_y_val = f.create_dataset('snli_testing_labels', data=snli_testing_labels)\n",
    "if not 'mnli_testing_data' in f: mnli_x_val = f.create_dataset('mnli_testing_data', data=mnli_testing_data)\n",
    "if not 'mnli_testing_labels' in f: mnli_y_val = f.create_dataset('mnli_testing_labels', data=mnli_testing_labels)\n",
    "    \n",
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))\n",
    "f.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an embedding matrix (the first layer of later models) and store it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_NB_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c951b659c774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing embedding matrix.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# prepare embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: is this correct? In the example, the first dimension is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_NB_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand\n",
    "\n",
    "#save as a model for later use\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "\n",
    "embedding_model = Model(input=sentence_input, output=embedded_sentence)\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_model(embedding_model, '../data/embedding_layer.hdf5')\n",
    "print(embedding_layer.input_dim)\n",
    "print(embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "(None, 100)\n",
      "(None, 100, 300)\n",
      "(1, 91461, 300)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          27438300  \n",
      "=================================================================\n",
      "Total params: 27,438,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "embedding_model = load_model(LOCAL_MODEL_PATH + 'embedding_layer.hdf5')\n",
    "print('Done.')\n",
    "print(embedding_model.input_shape)\n",
    "print(embedding_model.output_shape)\n",
    "print(np.shape(embedding_model.get_weights()))\n",
    "print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most simple model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_model(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model1 = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same model but using a shared architecture for embedding both sentences, so that it will profit from all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(512, activation='relu')(flat)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "conv_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 22, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 5, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "=================================================================\n",
      "Total params: 27,652,188\n",
      "Trainable params: 213,888\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 1, 128)        27652188                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 256)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           131584                                       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            32832                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4)             260                                          \n",
      "====================================================================================================\n",
      "Total params: 27,816,864\n",
      "Trainable params: 378,564\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_model.summary()\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for later: why doesn't convolution change the output size? Shouldnt it go down from to 100 to 98? -> solved, was because of bordermode = \"same\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Batches per epoch training: 3677\n",
      "Batches per epoch validation: 39\n",
      "Training the model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=3677, validation_data=<generator..., verbose=2, epochs=10, validation_steps=39)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277s - loss: 0.7022 - acc: 0.7001 - val_loss: 0.9628 - val_acc: 0.6417\n",
      "Epoch 2/10\n",
      "276s - loss: 0.6846 - acc: 0.7105 - val_loss: 0.9441 - val_acc: 0.6530\n",
      "Epoch 3/10\n",
      "276s - loss: 0.6667 - acc: 0.7199 - val_loss: 0.9145 - val_acc: 0.6642\n",
      "Epoch 4/10\n",
      "276s - loss: 0.6522 - acc: 0.7278 - val_loss: 0.9196 - val_acc: 0.6655\n",
      "Epoch 5/10\n",
      "275s - loss: 0.6402 - acc: 0.7339 - val_loss: 0.9158 - val_acc: 0.6704\n",
      "Epoch 6/10\n",
      "275s - loss: 0.6297 - acc: 0.7397 - val_loss: 0.9111 - val_acc: 0.6750\n",
      "Epoch 7/10\n",
      "276s - loss: 0.6207 - acc: 0.7443 - val_loss: 0.9108 - val_acc: 0.6799\n",
      "Epoch 8/10\n",
      "275s - loss: 0.6129 - acc: 0.7483 - val_loss: 0.9041 - val_acc: 0.6861\n",
      "Epoch 9/10\n",
      "275s - loss: 0.6055 - acc: 0.7520 - val_loss: 0.9030 - val_acc: 0.6838\n",
      "Epoch 10/10\n",
      "275s - loss: 0.5987 - acc: 0.7552 - val_loss: 0.8948 - val_acc: 0.6875\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "       \n",
    "batch_size = 256\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of batches until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Batches per epoch training: {}\".format(samples_per_epoch))\n",
    "print(\"Batches per epoch validation: {}\".format(nb_val_samples))\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "conv_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          steps_per_epoch = samples_per_epoch,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"saving the model...\")\n",
    "conv_model.save_weights(LOCAL_MODEL_PATH + 'conv_model_snli+mnli.hdf5')\n",
    "conv_model.save_weights(HARD_DRIVE_MODEL_PATH + 'conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weights...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model weights...\")\n",
    "conv_model.load_weights(HARD_DRIVE_MODEL_PATH + 'conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataset, batch_size=256, nb_epochs=10):\n",
    "    print(\"Loading training and validation data...\")\n",
    "    f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "    if dataset == 'snli':\n",
    "        x_val = f['snli_testing_data']\n",
    "        y_val = f['snli_testing_labels']\n",
    "    elif dataset == 'mnli':\n",
    "        x_val = f['mnli_testing_data']\n",
    "        y_val = f['mnli_testing_labels']\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Evaluating on the snli data\")\n",
    "    nb_val_samples = int(np.floor(np.shape(y_val)[0] / batch_size))\n",
    "    score, acc = model.evaluate_generator(val_data_generator(x_val, y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "                                steps = nb_val_samples)\n",
    "    print(\"Score: {}, Accuracy: {}\".format(score, acc))\n",
    "    f.close()\n",
    "    return score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the snli data\n",
      "Score: 0.8947591491234608, Accuracy: 0.6875\n",
      "SNLI-results: score = 0.8947591491234608, acc = 0.6875\n",
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the snli data\n",
      "Score: 1.053122070274855, Accuracy: 0.6121504934210527\n",
      "MNLI-results: score = 1.053122070274855, acc = 0.6121504934210527\n"
     ]
    }
   ],
   "source": [
    "score, acc = eval_model(conv_model, \"snli\")\n",
    "print(\"SNLI-results: score = {}, acc = {}\".format(score, acc))\n",
    "score, acc = eval_model(conv_model, \"mnli\")\n",
    "print(\"MNLI-results: score = {}, acc = {}\".format(score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Convolutional Model that also returns the encoded sentences as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "alt_conv_model = Model(inputs=[sentence1_input, sentence2_input], outputs=[preds, sentence1_embedding, sentence2_embedding])\n",
    "alt_conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"loading model weights...\")\n",
    "alt_conv_model.load_weights('../models/conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import *\n",
    "LSTM_UNITS = 64    \n",
    "SINGLE_ATTENTION_VECTOR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention model experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:31: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, LSTM, Bidirectional, Dropout, TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#1. Embed\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "\n",
    "#2. Encode\n",
    "# see keras example imdb_cnn_lstm\n",
    "encoded_sentence = Dropout(0.25)(sentence_embedding)\n",
    "\n",
    "#should output 64 feature maps with 104(narrow) or 106(wide) dims each\n",
    "encoded_sentence = Conv1D(64, kernel_size=5, activation=\"relu\", padding=\"valid\", strides=1)(encoded_sentence)\n",
    "encoded_sentence = MaxPooling1D(pool_size=4)(encoded_sentence)\n",
    "encoded_sentence = Permute((2,1))(encoded_sentence)\n",
    "encoded_sentence = Bidirectional(LSTM(24, activation='relu', return_sequences=True))(encoded_sentence)\n",
    "encoded_sentence = Bidirectional(LSTM(24, activation='relu', return_sequences=True))(encoded_sentence)\n",
    "output_lstm = Permute((2,1), name=\"output_lstm\")(encoded_sentence)\n",
    "\n",
    "#3. Attend\n",
    "#compute importance of sequences\n",
    "attention = TimeDistributed(Dense(1, activation='tanh'), name='importances')(output_lstm)\n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax', name=\"alphas\")(attention)\n",
    "attention = RepeatVector(64)(attention)\n",
    "attention = Permute((2,1))(attention)\n",
    "\n",
    "#elemwise mult with sequences and then weighte sum\n",
    "encoded_sentence = merge([output_lstm, attention], mode='mul', name=\"mul_with_alpha\")\n",
    "encoded_sentence = Lambda(lambda x: K.sum(x, axis=1, keepdims=False), output_shape=(64,), name=\"weighted_sum\")(encoded_sentence)\n",
    "\n",
    "lstm_sentence_encoding_model = Model(inputs=sentence_input, outputs=encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 64)            96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "permute_3 (Permute)          (None, 64, 24)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64, 48)            9408      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 64, 48)            14016     \n",
      "_________________________________________________________________\n",
      "output_lstm (Permute)        (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "importances (TimeDistributed (None, 48, 1)             65        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "alphas (Activation)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 64, 48)            0         \n",
      "_________________________________________________________________\n",
      "permute_4 (Permute)          (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "mul_with_alpha (Merge)       (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "weighted_sum (Lambda)        (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 27,557,853\n",
      "Trainable params: 119,553\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_sentence_encoding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = lstm_sentence_encoding_model(sentence1_input)\n",
    "sentence2_embedding = lstm_sentence_encoding_model(sentence2_input)\n",
    "\n",
    "#4. Predict\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "#flat = Flatten()(merged_vector)\n",
    "#merged_vector = Dropout(0.5)(merged_vector)\n",
    "x = Dense(512, activation='relu')(merged_vector)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(4, activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "clip_adam = Adam(clipnorm=1.)\n",
    "lstm_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "lstm_model.compile(loss='mse',\n",
    "             optimizer=clip_adam,\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_3 (Model)                  (None, 64)            27557853                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 512)           66048                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            32832                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             260                                          \n",
      "====================================================================================================\n",
      "Total params: 27,656,993\n",
      "Trainable params: 218,693\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Training the Reccurent model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=7354, validation_data=<generator..., verbose=2, epochs=10, validation_steps=78)`\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of batches until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Training the Reccurent model...\")\n",
    "\n",
    "lstm_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          steps_per_epoch = samples_per_epoch,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving recurrent model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"saving recurrent model...\")\n",
    "#lstm_model.save_weights(LOCAL_MODEL_PATH + 'lstm_model_weights_attention.hdf5')\n",
    "lstm_model.save_weights(HARD_DRIVE_MODEL_PATH + 'lstm_model_weights_attention.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weights...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model weights...\") \n",
    "lstm_model.load_weights(HARD_DRIVE_MODEL_PATH + '/lstm_model_weights_attention.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the snli data\n",
      "Score: 0.09323738706417573, Accuracy: 0.7357772435897436\n",
      "SNLI-results: score = 0.09323738706417573, acc = 0.7357772435897436\n",
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the snli data\n",
      "Score: 0.11954225795833688, Accuracy: 0.6439144736842105\n",
      "MNLI-results: score = 0.11954225795833688, acc = 0.6439144736842105\n"
     ]
    }
   ],
   "source": [
    "score, acc = eval_model(lstm_model, \"snli\");\n",
    "print(\"SNLI-results: score = {}, acc = {}\".format(score, acc))\n",
    "score, acc = eval_model(lstm_model, \"mnli\");\n",
    "print(\"MNLI-results: score = {}, acc = {}\".format(score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not too bad! Next up:\n",
    "- improving the convolution architecture (num filters, maxpooling size)\n",
    "- trying out lstms for encoding the sentences\n",
    "- testing with the actual repeval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A recursive network implementation in theano: https://github.com/ofirnachum/tree_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error Analysis\n",
    "\n",
    "Analyze which sentences are classified wrong and why:\n",
    "- print out target, prediction, sentence 1 and sentence 2 of wrongly classified samples (save to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading word index...\")\n",
    "word_index = pickle.load(open(\"../data/word_index.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of snli samples: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli targets: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "wrong_classifications = []\n",
    "correct_classifications = []\n",
    "\n",
    "\n",
    "index_to_word = {v: k for k, v in word_index.items()} #maps from integer word indices to the words themselves\n",
    "label_to_word = {0: 'neutral', 1: 'contradiction', 2:'entailment', 3:'other'}\n",
    "\n",
    "num_samples = np.shape(mnli_x_val)[1]\n",
    "                 \n",
    "for i in range(num_samples):\n",
    "    sequence1 = np.reshape(mnli_x_val[0,i,:], (1,100))\n",
    "    sequence2 = np.reshape(mnli_x_val[1,i,:], (1,100))\n",
    "    y_pred = np.argmax(conv_model.predict([sequence1, sequence2]))\n",
    "    y_true = np.argmax(mnli_y_val[i])\n",
    "    #print(y_pred);print(y_true)\n",
    "    \n",
    "    sentence1 = \"\"\n",
    "    for index in sequence1[0]:\n",
    "        if index != 0: \n",
    "            sentence1 += index_to_word[index] + \" \"\n",
    "    sentence2 = \"\"\n",
    "    for index in sequence2[0]:\n",
    "        if index != 0: \n",
    "            sentence2 += index_to_word[index] + \" \"\n",
    "            \n",
    "    if y_pred != y_true:            \n",
    "        wrong_classifications.append((y_true, y_pred, sentence1, sentence2))\n",
    "    elif y_pred == y_true:\n",
    "        correct_classifications.append((y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "print(\"Found {} correct and {} false classifications.\".format(len(correct_classifications),len(wrong_classifications)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('../results/correct_classifications.txt', \"w\") as file:\n",
    "    for y_true, y_false, sentence1, sentence2 in correct_classifications:\n",
    "        file.write('----------------------\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_false], sentence1, sentence2))\n",
    "\n",
    "\n",
    "with open('../results/classification_errors.txt', \"w\") as file:\n",
    "    for y_true, y_false, sentence1, sentence2 in wrong_classifications:\n",
    "        file.write('----------------------\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_false], sentence1, sentence2))\n",
    "\n",
    "                 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How does a sentence need to change for a change in the output of the model?\n",
    "TODO: \n",
    "- write a function that turns one sentence into the right format and then computes the output of the model\n",
    "- test the different things:\n",
    "    - permutations\n",
    "    - negation\n",
    "    - exclude trigrams from the sentence\n",
    "- do this for 100 correctly classified and 100 wrongly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = pickle.load(open(\"../data/tokenizer.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_strings(string1, string2, tokenizer, model):\n",
    "    sequences = tokenizer.texts_to_sequences([string1, string2])\n",
    "    padded_sequences = simple_padding(sequences)\n",
    "    input_1 = np.reshape(np.array(padded_sequences[0]), (1,100))\n",
    "    input_2 = np.reshape(np.array(padded_sequences[1]), (1,100))\n",
    "    output_model = model.predict([input_1, input_2])\n",
    "    if len(output_model) == 1:\n",
    "        y_pred = np.argmax(output_model)\n",
    "        return y_pred\n",
    "    elif len(output_model) == 3:\n",
    "        y_pred = np.argmax(output_model[0])\n",
    "        return y_pred, output_model[1], output_model[2]\n",
    "    \n",
    "label_conv_dict = {\n",
    "    'neutral' : 0,\n",
    "    'contradiction' : 1,\n",
    "    'entailment' : 2,\n",
    "    'other' : 3,\n",
    "    0 : 'neutral',\n",
    "    1 : 'contradiction',\n",
    "    2 : 'entailment',\n",
    "    3 : 'other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_conv_dict[predict_strings(\"john not love mary\", \"john knows mary\", tokenizer, conv_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#small test of the alternative conv model\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(\"john not love mary\", \"john knows mary\", tokenizer, alt_conv_model)\n",
    "print(label_conv_dict[y_true])\n",
    "print(sentence1_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.choice(correct_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, ngrams\n",
    "import re\n",
    "def exclude_trigrams(string):\n",
    "    toks = word_tokenize(string)\n",
    "    trigrams = ngrams(toks, 3)\n",
    "    corrupted_list = []\n",
    "    for tri in trigrams:\n",
    "        reduced_string = string\n",
    "        for i in range(0, 3):\n",
    "            regex = r\"\\b({})\\b\".format(tri[i])\n",
    "            reduced_string = re.sub(regex, '', reduced_string)\n",
    "        corrupted_list.append(reduced_string)\n",
    "    return corrupted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = \"this is just a test sentence, it does not really mean anything\"\n",
    "print(exclude_trigrams(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2):\n",
    "    results = []  #a list of target, pred, sentence1, sentence2 with corruptions\n",
    "    #exlclude all possible trigrams\n",
    "    corrupted_sentence1 = exclude_trigrams(sentence1)\n",
    "    corrupted_sentence2 = exclude_trigrams(sentence2)\n",
    "    for corrupted_sentence in corrupted_sentence1:\n",
    "        corrupted_pred = predict_strings(corrupted_sentence, sentence2, tokenizer, conv_model)\n",
    "        changed = False\n",
    "        if corrupted_pred != y_pred:\n",
    "            changed = True\n",
    "        results.append((y_true, corrupted_pred, changed, corrupted_sentence, sentence2))\n",
    "    for corrupted_sentence in corrupted_sentence2:\n",
    "        corrupted_pred = predict_strings(sentence1, corrupted_sentence, tokenizer, conv_model)\n",
    "        changed = False\n",
    "        if corrupted_pred != y_pred:\n",
    "            changed = True\n",
    "        results.append((y_true, corrupted_pred, changed, sentence1, corrupted_sentence))\n",
    "    \n",
    "    return {'target' : y_true, 'pred' : y_pred, 'sentence1':sentence1, 'sentence2':sentence2,\n",
    "                         'corruptions' : results}\n",
    "NUM_SAMPLES = 100\n",
    "corruption_results = []\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    #take NUM_SAMPLES correct and NUM_SAMPLES wrong classifications\n",
    "    y_true, y_pred, sentence1, sentence2 = random.choice(correct_classifications)\n",
    "    corruption_results.append(apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "    #the same for wrong classifications\n",
    "    y_true, y_pred, sentence1, sentence2 = random.choice(wrong_classifications)\n",
    "    corruption_results.append(apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "print(len(corruption_results))\n",
    "print(corruption_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print results to file\n",
    "with open('../results/corrupted_sentences.txt', \"w\") as file:\n",
    "    for sample_results in corruption_results:\n",
    "        y_true = sample_results['target']\n",
    "        y_pred = sample_results['pred']\n",
    "        sentence1 = sample_results['sentence1']\n",
    "        sentence2 = sample_results['sentence2']\n",
    "        file.write('########################################\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_pred], sentence1, sentence2))\n",
    "        for (y_true, corrupted_pred, changed, corrupted_sentence, sentence2) in sample_results['corruptions']:\n",
    "            file.write('\\t--------\\n\\tTARGET: {}, PRED: {}, CHANGED = {}\\n\\tSENTENCE1: {}\\n\\tSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[corrupted_pred], changed, corrupted_sentence, sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring spaCy dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_depency(string):\n",
    "    doc = nlp(string)\n",
    "    dep_labels = []\n",
    "    for token in doc:\n",
    "        #print(token, token.dep_, token.pos_)\n",
    "        dep_labels.append((token, token.dep_, token.pos_))\n",
    "    return dep_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_negation(string):\n",
    "    parsed_string = parse_depency(string)\n",
    "    result = ''\n",
    "    for tok, dep, pos in parsed_string:\n",
    "        result += tok.text + ' '\n",
    "        if pos == 'VERB':\n",
    "            result += 'not '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parse_depency('i am fabian and i like butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "naive_negation('i am fabian and i like butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as scp\n",
    "\n",
    "sentence1 = \"i am fabian and i like butter\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence1, sentence1, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence1))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n",
    "print()\n",
    "\n",
    "sentence2 = \"i am fabian and i not like butter\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence2, sentence1, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence2))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n",
    "print()\n",
    "\n",
    "sentence3 = \"some test sentence for comparison\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence1, sentence3, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence3))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the model does what it should. As the sentence is negated, the predicted entailment changes, and the distance between the sentence encodings also increases but is comparably small regarding the distance to a completely different sentence encoding. \n",
    "\n",
    "Even more interesting to me is that in the third example, the model votes for entailment although the difference in embedding space is much higher than in example three. Seems like the classification model (just fully connected layers) is too weak for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
