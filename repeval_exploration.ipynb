{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/snli_1.0/'\n",
    "data_frame = pd.read_csv(data_path + 'snli_1.0_train.txt', sep='\\t')\n",
    "gold_labels = data_frame.gold_label.tolist()\n",
    "sentences1 = data_frame.sentence1.tolist()\n",
    "sentences2 = data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20000 samples from the dataset\n"
     ]
    }
   ],
   "source": [
    "#num_samples = len(sentences1)\n",
    "num_samples = 20000\n",
    "samples = [(sentences1[i], sentences2[i], gold_labels[i]) for i in range(num_samples)]\n",
    "print(\"Using {} samples from the dataset\".format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "max_length_sentence = np.max([len(sentence.split()) for sentence, sentence2, labels in samples])\n",
    "print(max_length_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, vectorspace=vectorspace):\n",
    "    result = np.array([])\n",
    "    missing_words = []\n",
    "    for word in sentence.split():  #TODO apply better tokenizing\n",
    "        try:\n",
    "            result = np.concatenate((result, vectorspace[word]))\n",
    "        except:\n",
    "            print(\"Missing {}\".format(word))\n",
    "            missing_words.append(word)\n",
    "    #print(np.shape(result))\n",
    "    #print(len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8273 unique tokens.\n",
      "Shape of data tensor: (2, 20000, 1000)\n",
      "Shape of label tensor: (20000, 4)\n",
      "Shape of x_train: (2, 16000, 1000)\n",
      "Shape of y_train: (16000, 4)\n",
      "Shape of x_val: (2, 4000, 1000)\n",
      "Shape of y_val: (4000, 4)\n",
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (8274, 300)\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [sentence for sentence, sentence2, label in samples]\n",
    "sentences2 = [sentence2 for sentence, sentence2, label in samples]\n",
    "labels = [label for sentence, sentence2, label in samples] \n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences1 + sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data1 = sequence.pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = sequence.pad_sequences(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "data = np.asarray([data1, data2])\n",
    "\n",
    "labels = to_categorical(np.asarray(numeric_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[1])\n",
    "np.random.shuffle(indices)\n",
    "data = data[:,indices,:]\n",
    "labels = labels[indices,:]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "x_train = data[:,:-num_validation_samples,:]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[:,-num_validation_samples:,:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo next:\n",
    "+ find out how to use RAM more efficiently: this model is eating RAM like a monster\n",
    "+ build model architecture that uses the same layers for encoding both sentences (starting with the tutorial used above)\n",
    "+ build model on top to predict entailment, neutral, contradiction (see intro functional, shared layers)\n",
    "+ test different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_layer(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 300)     2482200     input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_7 (Convolution1D)  (None, 996, 128)      192128      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      192128      embedding_2[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_7 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_8 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_8 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_9 (Convolution1D)  (None, 35, 128)       82048       maxpooling1d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_9 (MaxPooling1D)    (None, 1, 128)        0           convolution1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 1, 256)        0           maxpooling1d_9[0][0]             \n",
      "                                                                   maxpooling1d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 256)           0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 256)           65792       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4)             1028        dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,261,468\n",
      "Trainable params: 779,268\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55s - loss: 1.0836 - acc: 0.4209 - val_loss: 1.0228 - val_acc: 0.4715\n",
      "Epoch 2/5\n",
      "55s - loss: 1.0111 - acc: 0.4866 - val_loss: 1.0109 - val_acc: 0.4831\n",
      "Epoch 3/5\n",
      "55s - loss: 0.9733 - acc: 0.5181 - val_loss: 1.0242 - val_acc: 0.4831\n",
      "Epoch 4/5\n",
      "56s - loss: 0.9289 - acc: 0.5539 - val_loss: 1.0548 - val_acc: 0.4808\n",
      "Epoch 5/5\n",
      "55s - loss: 0.8737 - acc: 0.5975 - val_loss: 1.1142 - val_acc: 0.4713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa777e420b8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO use generators more wisely, introduce batching, look for fitting sizes\n",
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    #num_batches = int(np.floor(np.shape(y_train)[0] / batch_size))\n",
    "    #print(num_batches)\n",
    "    #print(num_batches*batch_size)\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        #print(i, np.shape(gen_output))\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        #print(i)\n",
    "        yield gen_output\n",
    "        \n",
    "#    for i in np.arange(0,num_batches*batch_size, batch_size):\n",
    "#        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "#        print(i, np.shape(gen_output))\n",
    "#        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    #num_batches = int(np.floor(np.shape(y_test)[0] / batch_size))\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        #print(i)\n",
    "        yield gen_output\n",
    "        \n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of steps until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(y_val)[0] / batch_size))\n",
    "\n",
    "model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=5,\n",
    "          samples_per_epoch = samples_per_epoch * batch_size,\n",
    "          validation_data=val_data_generator(x_val, y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples * batch_size,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??Model.fit_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And there we have overfitting! \n",
    "TODO:\n",
    "- get it working on the complete dataset (not just 20000)\n",
    "- try different models\n",
    "- prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
