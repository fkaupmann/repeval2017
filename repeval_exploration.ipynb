{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/snli_1.0/'\n",
    "data_frame = pd.read_csv(data_path + 'snli_1.0_train.txt', sep='\\t')\n",
    "gold_labels = data_frame.gold_label.tolist()\n",
    "sentences1 = data_frame.sentence1.tolist()\n",
    "sentences2 = data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20000 samples from the dataset\n"
     ]
    }
   ],
   "source": [
    "#num_samples = len(sentences1)\n",
    "num_samples = 20000\n",
    "samples = [(sentences1[i], sentences2[i], gold_labels[i]) for i in range(num_samples)]\n",
    "print(\"Using {} samples from the dataset\".format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8273 unique tokens.\n",
      "Shape of data tensor: (2, 20000, 1000)\n",
      "Shape of label tensor: (20000, 4)\n",
      "Shape of x_train: (2, 16000, 1000)\n",
      "Shape of y_train: (16000, 4)\n",
      "Shape of x_val: (2, 4000, 1000)\n",
      "Shape of y_val: (4000, 4)\n",
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (8274, 300)\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [sentence for sentence, sentence2, label in samples]\n",
    "sentences2 = [sentence2 for sentence, sentence2, label in samples]\n",
    "labels = [label for sentence, sentence2, label in samples] \n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences1 + sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data1 = sequence.pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = sequence.pad_sequences(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "data = np.asarray([data1, data2])\n",
    "\n",
    "labels = to_categorical(np.asarray(numeric_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[1])\n",
    "np.random.shuffle(indices)\n",
    "data = data[:,indices,:]\n",
    "labels = labels[indices,:]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "x_train = data[:,:-num_validation_samples,:]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[:,-num_validation_samples:,:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo next:\n",
    "+ find out how to use RAM more efficiently: this model is eating RAM like a monster\n",
    "+ build model architecture that uses the same layers for encoding both sentences (starting with the tutorial used above)\n",
    "+ build model on top to predict entailment, neutral, contradiction (see intro functional, shared layers)\n",
    "+ test different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_layer(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model1 = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 300)     2482200     input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      192128      embedding_1[3][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 996, 128)      192128      embedding_1[4][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_14 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_14 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_15 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_15 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 1, 256)        0           maxpooling1d_12[0][0]            \n",
      "                                                                   maxpooling1d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 256)           0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           65792       flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             1028        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,261,468\n",
      "Trainable params: 779,268\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but using the same architecture for embedding both sentences, so that it will profit from all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128 #just for testing \n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_layer(sentence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(35)(x)\n",
    "\n",
    "sentence_embedding_model = Model(input=sentence_input, output=encoded_sentence)\n",
    "\n",
    "\n",
    "#sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "#not sure whether jsut using sentence input multiple times works\n",
    "# or you need to specify different inputs explicitly\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 300)     2482200     input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_7 (Convolution1D)  (None, 996, 128)      192128      embedding_1[2][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_7 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_8 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_8 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_9 (Convolution1D)  (None, 35, 128)       82048       maxpooling1d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_9 (MaxPooling1D)    (None, 1, 128)        0           convolution1d_9[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 2,838,424\n",
      "Trainable params: 356,224\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_5 (Model)                  (None, 1, 128)        2838424     input_9[0][0]                    \n",
      "                                                                   input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 1, 256)        0           model_5[1][0]                    \n",
      "                                                                   model_5[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 256)           0           merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             1028        dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,905,244\n",
      "Trainable params: 423,044\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected input_9 to have shape (None, 128) but got array with shape (128, 1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6129ffd106e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           verbose=2)\n\u001b[0m",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m   1030\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: expected input_9 to have shape (None, 128) but got array with shape (128, 1000)"
     ]
    }
   ],
   "source": [
    "#TODO use generators more wisely, introduce batching, look for fitting sizes\n",
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    #num_batches = int(np.floor(np.shape(y_train)[0] / batch_size))\n",
    "    #print(num_batches)\n",
    "    #print(num_batches*batch_size)\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        #print(i, np.shape(gen_output))\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        #print(i)\n",
    "        yield gen_output\n",
    "        \n",
    "#    for i in np.arange(0,num_batches*batch_size, batch_size):\n",
    "#        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "#        print(i, np.shape(gen_output))\n",
    "#        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    #num_batches = int(np.floor(np.shape(y_test)[0] / batch_size))\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        #print(i)\n",
    "        yield gen_output\n",
    "        \n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of steps until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(y_val)[0] / batch_size))\n",
    "\n",
    "model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=5,\n",
    "          samples_per_epoch = samples_per_epoch * batch_size,\n",
    "          validation_data=val_data_generator(x_val, y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples * batch_size,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1000)\n"
     ]
    }
   ],
   "source": [
    "gen = training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size)\n",
    "print(np.shape(next(gen)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??Model.fit_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And there we have overfitting! \n",
    "TODO:\n",
    "- get it working on the complete dataset (not just 20000)\n",
    "- try different models\n",
    "- prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
