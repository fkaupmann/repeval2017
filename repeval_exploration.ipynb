{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/snli_1.0/'\n",
    "data_frame = pd.read_csv(data_path + 'snli_1.0_train.txt', sep='\\t')\n",
    "gold_labels = data_frame.gold_label.tolist()\n",
    "sentences1 = data_frame.sentence1.tolist()\n",
    "sentences2 = data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples = 30\n",
    "samples = [(sentences1[i], sentences2[i], gold_labels[i]) for i in range(num_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(num_layers, input_first_layer, filters):\n",
    "    inputs = []\n",
    "    inputs.append(input_first_layer)\n",
    "    for i in range(num_layers):\n",
    "        output_conv = Conv1D(nb_filter=filters, filter_length=3, activation='relu')(inputs[i])\n",
    "        inputs.append(output_conv)\n",
    "    output = MaxPooling1D()(inputs[i+1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size = (10,128)):\n",
    "    inputs = Input(shape=input_size)\n",
    "    outputs = conv_block(1, inputs, 64)\n",
    "    \n",
    "    model = Model(input=inputs, output=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 10, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 8, 64)         24640       input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 4, 64)         0           convolution1d_5[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 24,640\n",
      "Trainable params: 24,640\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "#todo use word embeddings instead of one-hot\n",
    "max_length_sentence = np.max([len(sentence.split()) for sentence, sentence2, labels in samples])\n",
    "print(max_length_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, vectorspace=vectorspace):\n",
    "    result = np.array([])\n",
    "    missing_words = []\n",
    "    for word in sentence.split():  #TODO apply better tokenizing\n",
    "        try:\n",
    "            result = np.concatenate((result, vectorspace[word]))\n",
    "        except:\n",
    "            print(\"Missing {}\".format(word))\n",
    "            missing_words.append(word)\n",
    "    #print(np.shape(result))\n",
    "    #print(len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 163 unique tokens.\n",
      "Shape of data tensor: (60, 1000)\n",
      "Shape of label tensor: (60, 3)\n",
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (164, 300)\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence for sentence, sentence2, label in samples] + [sentence2 for sentence, sentence2, label in samples]\n",
    "labels = [label for sentence, sentence2, label in samples] * 2 #TODO this doesn't make any sense, just for testing\n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "        \n",
    "tokenizer = text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(numeric_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo next:\n",
    "+ build model architecture that uses the same layers for encoding both sentences (starting with the tutorial used above)\n",
    "+ build model on top to predict entailment, neutral, contradiction (see intro functional, shared layers)\n",
    "+ test different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
