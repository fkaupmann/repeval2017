{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '2454' (I am process '3483')\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "snli_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_train.txt', sep='\\t')\n",
    "snli_gold_labels = snli_data_frame.gold_label.tolist()\n",
    "snli_sentences1 = snli_data_frame.sentence1.tolist()\n",
    "snli_sentences2 = snli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 24810: expected 15 fields, saw 16\\nSkipping line 33961: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 75911: expected 15 fields, saw 16\\nSkipping line 100114: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 150638: expected 15 fields, saw 16\\nSkipping line 158834: expected 15 fields, saw 16\\nSkipping line 173104: expected 15 fields, saw 16\\nSkipping line 178252: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 221951: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 286845: expected 15 fields, saw 16\\nSkipping line 314110: expected 15 fields, saw 16\\n'\n"
     ]
    }
   ],
   "source": [
    "mnli_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_train.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_gold_labels = mnli_data_frame.gold_label.tolist()\n",
    "mnli_sentences1 = mnli_data_frame.sentence1.tolist()\n",
    "mnli_sentences2 = mnli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 550152 training-samples from the snli-trainset\n",
      "Using 391165 training-samples from the multinli-trainset\n"
     ]
    }
   ],
   "source": [
    "snli_num_samples= len(snli_gold_labels)\n",
    "snli_samples = [(str(snli_sentences1[i]), str(snli_sentences2[i]), str(snli_gold_labels[i])) for i in range(snli_num_samples)]\n",
    "print(\"Using {} training-samples from the snli-trainset\".format(snli_num_samples))\n",
    "mnli_num_samples= len(mnli_gold_labels)\n",
    "mnli_samples = [(str(mnli_sentences1[i]), str(mnli_sentences2[i]), str(mnli_gold_labels[i])) for i in range(mnli_num_samples)]\n",
    "print(\"Using {} training-samples from the multinli-trainset\".format(mnli_num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnli_test_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_dev_matched.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_test_gold_labels = mnli_test_data_frame.gold_label.tolist()\n",
    "mnli_test_sentences1 = mnli_test_data_frame.sentence1.tolist()\n",
    "mnli_test_sentences2 = mnli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_test_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_test.txt', sep='\\t')\n",
    "snli_test_gold_labels = snli_test_data_frame.gold_label.tolist()\n",
    "snli_test_sentences1 = snli_test_data_frame.sentence1.tolist()\n",
    "snli_test_sentences2 = snli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10000 testing-samples from the snli-testset\n",
      "Using 9897 testing-samples from the multinli-testset\n"
     ]
    }
   ],
   "source": [
    "snli_test_num_samples= len(snli_test_gold_labels)\n",
    "snli_test_samples = [(str(snli_test_sentences1[i]), str(snli_test_sentences2[i]), str(snli_test_gold_labels[i])) for i in range(snli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the snli-testset\".format(snli_test_num_samples))\n",
    "mnli_test_num_samples= len(mnli_test_gold_labels)\n",
    "mnli_test_samples = [(str(mnli_test_sentences1[i]), str(mnli_test_sentences2[i]), str(mnli_test_gold_labels[i])) for i in range(mnli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the multinli-testset\".format(mnli_test_num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use h5py to store the data. This helps to use less RAM for the training data in training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 0.0 #there is some extra testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for padding without using numpy, which breaks when building data arrays with 500000 or more embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_padding(sequences, maxlen=100):\n",
    "    for sequence in sequences:\n",
    "        while len(sequence) < maxlen:\n",
    "            sequence.append(0)\n",
    "        if len(sequence) > maxlen:\n",
    "            del sequence[maxlen:]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn string sequences into integer sequences, pad them to equal length, divide into training and validation data, store using the hdf5 binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91460 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "str_sentences1 = [sentence for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "str_sentences2 = [sentence2 for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "labels = [label for sentence, sentence2, label in snli_samples + mnli_samples] \n",
    "\n",
    "snli_test_str_sentences1 = [sentence for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_labels = [label for sentence, sentence2, label in snli_test_samples] \n",
    "\n",
    "mnli_test_str_sentences1 = [sentence for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_labels = [label for sentence, sentence2, label in mnli_test_samples] \n",
    "\n",
    "numeric_labels = []\n",
    "for label in labels + snli_test_labels + mnli_test_labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(str_sentences1 + str_sentences2 + \n",
    "                       snli_test_str_sentences1 + snli_test_str_sentences2 +\n",
    "                      mnli_test_str_sentences1 + mnli_test_str_sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(str_sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(str_sentences2)\n",
    "snli_test_sequences1 = tokenizer.texts_to_sequences(snli_test_str_sentences1)\n",
    "snli_test_sequences2 = tokenizer.texts_to_sequences(snli_test_str_sentences2)\n",
    "mnli_test_sequences1 = tokenizer.texts_to_sequences(mnli_test_str_sentences1)\n",
    "mnli_test_sequences2 = tokenizer.texts_to_sequences(mnli_test_str_sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of sequences1: {}.\".format(np.shape(sequences1)))\n",
    "print(\"shape of sequences2: {}.\".format(np.shape(sequences2)))\n",
    "print(\"shape of snli_test_sequences1: {}.\".format(np.shape(snli_test_sequences1)))\n",
    "print(\"shape of snli_test_sequences2: {}.\".format(np.shape(snli_test_sequences2)))\n",
    "print(\"mhape of snli_test_sequences1: {}.\".format(np.shape(mnli_test_sequences1)))\n",
    "print(\"mhape of snli_test_sequences2: {}.\".format(np.shape(mnli_test_sequences2)))\n",
    "\n",
    "data1 = simple_padding(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = simple_padding(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data1 = simple_padding(snli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data2 = simple_padding(snli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data1 = simple_padding(mnli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data2 = simple_padding(mnli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"shape of data1: {}.\".format(np.shape(data1)))\n",
    "print(\"shape of data2: {}.\".format(np.shape(data2)))\n",
    "print(\"shape of snli_test_data1: {}.\".format(np.shape(snli_test_data1)))\n",
    "print(\"shape of snli_test_data2: {}.\".format(np.shape(snli_test_data2)))\n",
    "print(\"shape of mnli_test_data1: {}.\".format(np.shape(mnli_test_data1)))\n",
    "print(\"shape of mnli_test_data2: {}.\".format(np.shape(mnli_test_data2)))\n",
    "\n",
    "training_data = np.asarray([data1, data2])\n",
    "snli_testing_data = np.asarray([snli_test_data1, snli_test_data2])\n",
    "mnli_testing_data = np.asarray([mnli_test_data1, mnli_test_data2])\n",
    "\n",
    "all_labels = to_categorical(np.asarray(numeric_labels))\n",
    "training_labels = all_labels[:len(labels)]\n",
    "snli_testing_labels = all_labels[len(labels):len(labels) + len(snli_test_labels)]\n",
    "mnli_testing_labels = all_labels[len(labels) + len(snli_test_labels):]\n",
    "\n",
    "##########CONTINUE HERE###############\n",
    "\n",
    "print('Shape of training_data tensor:', training_data.shape)\n",
    "print('Shape of training_labels tensor:', training_labels.shape)\n",
    "print('Shape of snli_testing_data tensor:', snli_testing_data.shape)\n",
    "print('Shape of snli_testing_labels tensor:', snli_testing_labels.shape)\n",
    "print('Shape of mnli_testing_data tensor:', mnli_testing_data.shape)\n",
    "print('Shape of mnli_testing_labels tensor:', mnli_testing_labels.shape)\n",
    "\n",
    "# not needed here, because training and testing data are already splitted\n",
    "# split the data into a training set and a validation set\n",
    "#indices = np.arange(data.shape[1])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[:,indices,:]\n",
    "#labels = labels[indices,:]\n",
    "#num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "#x_train = data[:,:-num_validation_samples,:]\n",
    "#y_train = labels[:-num_validation_samples]\n",
    "#x_val = data[:,-num_validation_samples:,:]\n",
    "#y_val = labels[-num_validation_samples:]\n",
    "\n",
    "#print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "#print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "#print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "#print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print(\"Storing training and test data to hdf5...\")\n",
    "\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "if not 'training_data' in f: x_train = f.create_dataset('training_data', data=training_data)\n",
    "if not 'training_labels' in f: y_train = f.create_dataset('training_labels', data=training_labels)\n",
    "if not 'snli_testing_data' in f: snli_x_val = f.create_dataset('snli_testing_data', data=snli_testing_data)\n",
    "if not 'snli_testing_labels' in f: snli_y_val = f.create_dataset('snli_testing_labels', data=snli_testing_labels)\n",
    "if not 'mnli_testing_data' in f: mnli_x_val = f.create_dataset('mnli_testing_data', data=mnli_testing_data)\n",
    "if not 'mnli_testing_labels' in f: mnli_y_val = f.create_dataset('mnli_testing_labels', data=mnli_testing_labels)\n",
    "    \n",
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))\n",
    "f.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an embedding matrix (the first layer of later models) and store it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (91461, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=Reshape{3}...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "91461\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand\n",
    "\n",
    "#save as a model for later use\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "\n",
    "embedding_model = Model(input=sentence_input, output=embedded_sentence)\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_model(embedding_model, '../data/embedding_layer.hdf5')\n",
    "print(embedding_layer.input_dim)\n",
    "print(embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "(None, 100)\n",
      "(None, 100, 300)\n",
      "(1, 91461, 300)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          27438300  \n",
      "=================================================================\n",
      "Total params: 27,438,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "embedding_model = load_model('../data/embedding_layer.hdf5')\n",
    "print('Done.')\n",
    "print(embedding_model.input_shape)\n",
    "print(embedding_model.output_shape)\n",
    "print(np.shape(embedding_model.get_weights()))\n",
    "print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most simple model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_model(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model1 = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 300)     2482200     input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      192128      embedding_1[3][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 996, 128)      192128      embedding_1[4][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_14 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_14 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_15 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_15 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 1, 256)        0           maxpooling1d_12[0][0]            \n",
      "                                                                   maxpooling1d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 256)           0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           65792       flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             1028        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,261,468\n",
      "Trainable params: 779,268\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same model but using a shared architecture for embedding both sentences, so that it will profit from all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=Reshape{3}...)`\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "conv_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 22, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 5, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "=================================================================\n",
      "Total params: 27,652,188\n",
      "Trainable params: 213,888\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 1, 128)        27652188                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 256)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4)             1028                                         \n",
      "====================================================================================================\n",
      "Total params: 27,719,008\n",
      "Trainable params: 280,708\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_model.summary()\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for later: why doesn't convolution change the output size? Shouldnt it go down from to 100 to 98? -> solved, was because of bordermode = \"same\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Batches pro epoche training: 7354\n",
      "Batches pro epoche validation: 77\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=7354, validation_data=<generator..., verbose=2, epochs=10, validation_steps=77)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "276s - loss: 0.8259 - acc: 0.6272 - val_loss: 1.0685 - val_acc: 0.5619\n",
      "Epoch 2/10\n",
      "275s - loss: 0.7438 - acc: 0.6777 - val_loss: 1.0156 - val_acc: 0.6072\n",
      "Epoch 3/10\n",
      "274s - loss: 0.7074 - acc: 0.6980 - val_loss: 1.0064 - val_acc: 0.6156\n",
      "Epoch 4/10\n",
      "274s - loss: 0.6845 - acc: 0.7097 - val_loss: 1.0167 - val_acc: 0.6402\n",
      "Epoch 5/10\n",
      "273s - loss: 0.6674 - acc: 0.7188 - val_loss: 1.0357 - val_acc: 0.6410\n",
      "Epoch 6/10\n",
      "272s - loss: 0.6538 - acc: 0.7257 - val_loss: 0.9737 - val_acc: 0.6551\n",
      "Epoch 7/10\n",
      "272s - loss: 0.6420 - acc: 0.7318 - val_loss: 0.9768 - val_acc: 0.6572\n",
      "Epoch 8/10\n",
      "275s - loss: 0.6325 - acc: 0.7367 - val_loss: 0.9752 - val_acc: 0.6526\n",
      "Epoch 9/10\n",
      "275s - loss: 0.6244 - acc: 0.7408 - val_loss: 0.9748 - val_acc: 0.6614\n",
      "Epoch 10/10\n",
      "273s - loss: 0.6170 - acc: 0.7445 - val_loss: 0.9687 - val_acc: 0.6607\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "       \n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of batches until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(mnli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Batches pro epoche training: {}\".format(samples_per_epoch))\n",
    "print(\"Batches pro epoche validation: {}\".format(nb_val_samples))\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "conv_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          steps_per_epoch = samples_per_epoch,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n",
      "done,\n"
     ]
    }
   ],
   "source": [
    "print(\"saving the model...\")\n",
    "conv_model.save_weights('../models/conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "encoded_sentence = GRU(32, activation='relu')(sentence_embedding)\n",
    "#TODO: stack lstms here\n",
    "lstm_sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = lstm_sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = lstm_sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "#flat = Flatten()(merged_vector)\n",
    "x = Dense(64, activation='relu')(merged_vector)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "clip_adam = Adam(clipnorm=1.)\n",
    "lstm_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "lstm_model.compile(loss='mse',\n",
    "             optimizer=clip_adam,\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                31968     \n",
      "=================================================================\n",
      "Total params: 27,470,268\n",
      "Trainable params: 31,968\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_5 (Model)                  (None, 32)            27470268                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 64)            4160                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             260                                          \n",
      "====================================================================================================\n",
      "Total params: 27,474,688\n",
      "Trainable params: 36,388\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_sentence_embedding_model.summary()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Training the GRU model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., verbose=2, steps_per_epoch=941312, epochs=10, validation_steps=9984)`\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/fabian/.theano/compiledir_Linux-4.8--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.0-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d63ca4c22e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnli_x_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnli_y_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           verbose=2)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1876\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1619\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of steps until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Training the GRU model...\")\n",
    "\n",
    "lstm_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          samples_per_epoch = samples_per_epoch * batch_size,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples * batch_size,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I get nans as loss? -> exploding gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"saving gru model...\")\n",
    "lstm_model.save_weights('../data/lstm_model_weights.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not too bad! Next up:\n",
    "- improving the convolution architecture (num filters, maxpooling size)\n",
    "- trying out lstms for encoding the sentences\n",
    "- testing with the actual repeval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A recursive network implementation in theano: https://github.com/ofirnachum/tree_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error Analysis\n",
    "\n",
    "Analyze which sentences are classified wrong and why:\n",
    "- print out target, prediction, sentence 1 and sentence 2 of wrongly classified samples (save to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of snli samples: (2, 10000, 100)\n",
      "Shape of snli targets: (10000, 4)\n",
      "Shape of mnli samples: (2, 9897, 100)\n",
      "Shape of mnli samples: (9897, 4)\n",
      "Found 3869 false classifications.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of snli samples: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli targets: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "classification_errors = []\n",
    "\n",
    "index_to_word = {v: k for k, v in word_index.items()} #maps from integer word indices to the words themselves\n",
    "label_to_word = {0: 'neutral', 1: 'contradiction', 2:'entailment', 3:'other'}\n",
    "\n",
    "num_samples = np.shape(mnli_x_val)[1]\n",
    "                 \n",
    "for i in range(num_samples):\n",
    "    sequence1 = np.reshape(mnli_x_val[0,i,:], (1,100))\n",
    "    sequence2 = np.reshape(mnli_x_val[1,i,:], (1,100))\n",
    "    y_pred = np.argmax(conv_model.predict([sequence1, sequence2]))\n",
    "    y_true = np.argmax(mnli_y_val[i])\n",
    "    #print(y_pred);print(y_true)\n",
    "    if y_pred != y_true:\n",
    "        #TODO: reconstruct sentences from sequences\n",
    "        sentence1 = \"\"\n",
    "        for index in sequence1[0]:\n",
    "            if index != 0: sentence1 += index_to_word[index] + \" \"\n",
    "        sentence2 = \"\"\n",
    "        for index in sequence2[0]:\n",
    "            if index != 0: sentence2 += index_to_word[index] + \" \"\n",
    "        classification_errors.append((y_true, y_pred, sentence1, sentence2))\n",
    "\n",
    "print(\"Found {} false classifications.\".format(len(classification_errors)))\n",
    "                 \n",
    "with open('../results/classification_errors.txt', \"w\") as file:\n",
    "    for y_true, y_false, sentence1, sentence2 in classification_errors:\n",
    "        file.write('----------------------\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_false], sentence1, sentence2))\n",
    "\n",
    "                 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
