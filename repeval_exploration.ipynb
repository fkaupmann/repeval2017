{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/snli_1.0/'\n",
    "data_frame = pd.read_csv(data_path + 'snli_1.0_train.txt', sep='\\t')\n",
    "gold_labels = data_frame.gold_label.tolist()\n",
    "sentences1 = data_frame.sentence1.tolist()\n",
    "sentences2 = data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 550152 samples from the dataset\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(gold_labels)\n",
    "samples = [(str(sentences1[i]), str(sentences2[i]), str(gold_labels[i])) for i in range(num_samples)]\n",
    "print(\"Using {} samples from the dataset\".format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use h5py to store the data. This helps to use less RAM for the training data in training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 35000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for padding without using numpy, which breaks when building data arrays with 500000 or more embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_padding(sequences, maxlen=100):\n",
    "    for sequence in sequences:\n",
    "        while len(sequence) < maxlen:\n",
    "            sequence.append(0)\n",
    "        if len(sequence) > maxlen:\n",
    "            del sequence[maxlen:]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn string sequences into integer sequences, pad them to equal length, divide into training and validation data, store using the hdf5 binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34369 unique tokens.\n",
      "shape of sequences1: (550152,).\n",
      "shape of sequences2: (550152,).\n",
      "shape of data1: (550152, 100).\n",
      "shape of data21: (550152, 100).\n",
      "Shape of data tensor: (2, 550152, 100)\n",
      "Shape of label tensor: (550152, 4)\n",
      "Shape of x_train: (2, 440122, 100)\n",
      "Shape of y_train: (440122, 4)\n",
      "Shape of x_val: (2, 110030, 100)\n",
      "Shape of y_val: (110030, 4)\n",
      "Storing training and test data to hdf5...\n",
      "Shape of x_train: (2, 440122, 100)\n",
      "Shape of y_train: (440122, 4)\n",
      "Shape of x_val: (2, 110030, 100)\n",
      "Shape of y_val: (110030, 4)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "str_sentences1 = [sentence for sentence, sentence2, label in samples]\n",
    "str_sentences2 = [sentence2 for sentence, sentence2, label in samples]\n",
    "labels = [label for sentence, sentence2, label in samples] \n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(str_sentences1 + str_sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(str_sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(str_sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print(\"shape of sequences1: {}.\".format(np.shape(sequences1)))\n",
    "print(\"shape of sequences2: {}.\".format(np.shape(sequences2)))\n",
    "\n",
    "data1 = simple_padding(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = simple_padding(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"shape of data1: {}.\".format(np.shape(data1)))\n",
    "print(\"shape of data21: {}.\".format(np.shape(data2)))\n",
    "\n",
    "data = np.asarray([data1, data2])\n",
    "\n",
    "labels = to_categorical(np.asarray(numeric_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[1])\n",
    "np.random.shuffle(indices)\n",
    "data = data[:,indices,:]\n",
    "labels = labels[indices,:]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "x_train = data[:,:-num_validation_samples,:]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[:,-num_validation_samples:,:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print(\"Storing training and test data to hdf5...\")\n",
    "\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "if not 'x_train' in f: x_train = f.create_dataset('x_train', data=x_train)\n",
    "if not 'y_train' in f: y_train = f.create_dataset('y_train', data=y_train)\n",
    "if not 'x_val' in f: x_val = f.create_dataset('x_val', data=x_val)\n",
    "if not 'y_val' in f: y_val = f.create_dataset('y_val', data=y_val)\n",
    "    \n",
    "print(\"Shape of x_train: {}\".format(np.shape(f['x_train'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['y_train'])))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(f['x_val'])))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(f['y_val'])))\n",
    "f.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an embedding matrix (the first layer of later models) and store it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (34370, 300)\n",
      "Saving model...\n",
      "34370\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand\n",
    "\n",
    "#save as a model for later use\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "\n",
    "embedding_model = Model(input=sentence_input, output=embedded_sentence)\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_model(embedding_model, '../data/embedding_layer.hdf5')\n",
    "print(embedding_layer.input_dim)\n",
    "print(embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "(None, 100)\n",
      "(None, 100, 300)\n",
      "(1, 34370, 300)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 100, 300)      10311000    input_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 10,311,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 10,311,000\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "embedding_model = load_model('../data/embedding_layer.hdf5')\n",
    "print('Done.')\n",
    "print(embedding_model.input_shape)\n",
    "print(embedding_model.output_shape)\n",
    "print(np.shape(embedding_model.get_weights()))\n",
    "print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most simple model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_model(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model1 = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 300)     2482200     input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      192128      embedding_1[3][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 996, 128)      192128      embedding_1[4][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_14 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_14 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_15 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_15 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 1, 256)        0           maxpooling1d_12[0][0]            \n",
      "                                                                   maxpooling1d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 256)           0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           65792       flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             1028        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,261,468\n",
      "Trainable params: 779,268\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same model but using a shared architecture for embedding both sentences, so that it will profit from all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(input=sentence_input, output=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "conv_model = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 100, 300)      10311000    input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 98, 128)       115328      model_1[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 24, 128)       0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 22, 128)       49280       maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 7, 128)        0           convolution1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_6 (Convolution1D)  (None, 5, 128)        49280       maxpooling1d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_6 (MaxPooling1D)    (None, 1, 128)        0           convolution1d_6[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 10,524,888\n",
      "Trainable params: 213,888\n",
      "Non-trainable params: 10,311,000\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_3 (Model)                  (None, 1, 128)        10524888    input_5[0][0]                    \n",
      "                                                                   input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 1, 256)        0           model_3[1][0]                    \n",
      "                                                                   model_3[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 256)           0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 256)           65792       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4)             1028        dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 10,591,708\n",
      "Trainable params: 280,708\n",
      "Non-trainable params: 10,311,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_model.summary()\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for later: why doesn't convolution change the output size? Shouldnt it go down from to 100 to 98? -> solved, was because of bordermode = \"same\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 440122, 100)\n",
      "Shape of y_train: (440122, 4)\n",
      "Shape of x_val: (2, 110030, 100)\n",
      "Shape of y_val: (110030, 4)\n",
      "Training the model...\n",
      "Epoch 1/10\n",
      "144s - loss: 0.8143 - acc: 0.6347 - val_loss: 0.7518 - val_acc: 0.6797\n",
      "Epoch 2/10\n",
      "145s - loss: 0.6988 - acc: 0.7069 - val_loss: 0.6953 - val_acc: 0.7106\n",
      "Epoch 3/10\n",
      "144s - loss: 0.6453 - acc: 0.7345 - val_loss: 0.6708 - val_acc: 0.7247\n",
      "Epoch 4/10\n",
      "142s - loss: 0.6071 - acc: 0.7545 - val_loss: 0.6635 - val_acc: 0.7313\n",
      "Epoch 5/10\n",
      "142s - loss: 0.5786 - acc: 0.7683 - val_loss: 0.6656 - val_acc: 0.7322\n",
      "Epoch 6/10\n",
      "143s - loss: 0.5555 - acc: 0.7795 - val_loss: 0.6682 - val_acc: 0.7335\n",
      "Epoch 7/10\n",
      "142s - loss: 0.5356 - acc: 0.7883 - val_loss: 0.6779 - val_acc: 0.7319\n",
      "Epoch 8/10\n",
      "143s - loss: 0.5189 - acc: 0.7954 - val_loss: 0.6967 - val_acc: 0.7319\n",
      "Epoch 9/10\n",
      "142s - loss: 0.5037 - acc: 0.8032 - val_loss: 0.7092 - val_acc: 0.7293\n",
      "Epoch 10/10\n",
      "141s - loss: 0.4901 - acc: 0.8094 - val_loss: 0.7231 - val_acc: 0.7282\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_val = f['x_val']\n",
    "y_val = f['y_val']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "       \n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of steps until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "conv_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          samples_per_epoch = samples_per_epoch * batch_size,\n",
    "          validation_data=val_data_generator(x_val, y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples * batch_size,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights('../data/conv_model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "encoded_sentence = GRU(128, activation='relu')(sentence_embedding)\n",
    "#TODO: stack lstms here\n",
    "lstm_sentence_embedding_model = Model(input=sentence_input, output=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = lstm_sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = lstm_sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "#flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(merged_vector)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "clip_adam = Adam(clipnorm=1.)\n",
    "lstm_model = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "lstm_model.compile(loss='mse',\n",
    "             optimizer=clip_adam,\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 100, 300)      10311000    input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 128)           164736      model_1[3][0]                    \n",
      "====================================================================================================\n",
      "Total params: 10,475,736\n",
      "Trainable params: 164,736\n",
      "Non-trainable params: 10,311,000\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_4 (Model)                  (None, 128)           10475736    input_9[0][0]                    \n",
      "                                                                   input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 256)           0           model_4[1][0]                    \n",
      "                                                                   model_4[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             1028        dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 10,542,556\n",
      "Trainable params: 231,556\n",
      "Non-trainable params: 10,311,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_sentence_embedding_model.summary()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 440122, 100)\n",
      "Shape of y_train: (440122, 4)\n",
      "Shape of x_val: (2, 110030, 100)\n",
      "Shape of y_val: (110030, 4)\n",
      "Training the model...\n",
      "Epoch 1/10\n",
      "395s - loss: 0.1671 - acc: 0.3339 - val_loss: 0.1672 - val_acc: 0.3342\n",
      "Epoch 2/10\n",
      "393s - loss: 0.1669 - acc: 0.3344 - val_loss: 0.1670 - val_acc: 0.3342\n",
      "Epoch 3/10\n",
      "389s - loss: 0.1669 - acc: 0.3328 - val_loss: 0.1669 - val_acc: 0.3342\n",
      "Epoch 4/10\n",
      "389s - loss: 0.1669 - acc: 0.3329 - val_loss: 0.1669 - val_acc: 0.3342\n",
      "Epoch 5/10\n",
      "389s - loss: 0.1669 - acc: 0.3328 - val_loss: 0.1669 - val_acc: 0.3342\n",
      "Epoch 6/10\n",
      "388s - loss: 0.1669 - acc: 0.3331 - val_loss: 0.1669 - val_acc: 0.3342\n",
      "Epoch 7/10\n",
      "389s - loss: 0.1669 - acc: 0.3330 - val_loss: 0.1669 - val_acc: 0.3342\n",
      "Epoch 8/10\n",
      "388s - loss: 0.1669 - acc: 0.3329 - val_loss: 0.1669 - val_acc: 0.3330\n",
      "Epoch 9/10\n",
      "388s - loss: 0.1669 - acc: 0.3331 - val_loss: 0.1669 - val_acc: 0.3330\n",
      "Epoch 10/10\n",
      "389s - loss: 0.1669 - acc: 0.3331 - val_loss: 0.1669 - val_acc: 0.3330\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_val = f['x_val']\n",
    "y_val = f['y_val']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of steps until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "lstm_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          samples_per_epoch = samples_per_epoch * batch_size,\n",
    "          validation_data=val_data_generator(x_val, y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples * batch_size,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I get nans as loss? -> exploding gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.save_weights('../data/lstm_model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not too bad! Next up:\n",
    "- improving the convolution architecture (num filters, maxpooling size)\n",
    "- trying out lstms for encoding the sentences\n",
    "- testing with the actual repeval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A recursive network implementation in theano: https://github.com/ofirnachum/tree_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
