{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeval 2017 Exporations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import merge\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load pre-trained word embeddings. Here, the ones from Mikolov using the word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_BINARY_PATH = '../data/GoogleNews-vectors-negative300.bin'\n",
    "vectorspace = KeyedVectors.load_word2vec_format(W2V_BINARY_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Starting here with the SNLI Corpus until further data are available for the repeval2017 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "snli_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_train.txt', sep='\\t')\n",
    "snli_gold_labels = snli_data_frame.gold_label.tolist()\n",
    "snli_sentences1 = snli_data_frame.sentence1.tolist()\n",
    "snli_sentences2 = snli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 24810: expected 15 fields, saw 16\\nSkipping line 33961: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 75911: expected 15 fields, saw 16\\nSkipping line 100114: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 150638: expected 15 fields, saw 16\\nSkipping line 158834: expected 15 fields, saw 16\\nSkipping line 173104: expected 15 fields, saw 16\\nSkipping line 178252: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 221951: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 286845: expected 15 fields, saw 16\\nSkipping line 314110: expected 15 fields, saw 16\\n'\n"
     ]
    }
   ],
   "source": [
    "mnli_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_train.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_gold_labels = mnli_data_frame.gold_label.tolist()\n",
    "mnli_sentences1 = mnli_data_frame.sentence1.tolist()\n",
    "mnli_sentences2 = mnli_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use only part of the data for testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 550152 training-samples from the snli-trainset\n",
      "Using 391165 training-samples from the multinli-trainset\n"
     ]
    }
   ],
   "source": [
    "snli_num_samples= len(snli_gold_labels)\n",
    "snli_samples = [(str(snli_sentences1[i]), str(snli_sentences2[i]), str(snli_gold_labels[i])) for i in range(snli_num_samples)]\n",
    "print(\"Using {} training-samples from the snli-trainset\".format(snli_num_samples))\n",
    "mnli_num_samples= len(mnli_gold_labels)\n",
    "mnli_samples = [(str(mnli_sentences1[i]), str(mnli_sentences2[i]), str(mnli_gold_labels[i])) for i in range(mnli_num_samples)]\n",
    "print(\"Using {} training-samples from the multinli-trainset\".format(mnli_num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnli_test_data_frame = pd.read_csv(data_path + 'multinli_0.9/multinli_0.9_dev_matched.txt', sep='\\t', error_bad_lines=False)\n",
    "mnli_test_gold_labels = mnli_test_data_frame.gold_label.tolist()\n",
    "mnli_test_sentences1 = mnli_test_data_frame.sentence1.tolist()\n",
    "mnli_test_sentences2 = mnli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_test_data_frame = pd.read_csv(data_path + 'snli_1.0/snli_1.0_test.txt', sep='\\t')\n",
    "snli_test_gold_labels = snli_test_data_frame.gold_label.tolist()\n",
    "snli_test_sentences1 = snli_test_data_frame.sentence1.tolist()\n",
    "snli_test_sentences2 = snli_test_data_frame.sentence2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10000 testing-samples from the snli-testset\n",
      "Using 9897 testing-samples from the multinli-testset\n"
     ]
    }
   ],
   "source": [
    "snli_test_num_samples= len(snli_test_gold_labels)\n",
    "snli_test_samples = [(str(snli_test_sentences1[i]), str(snli_test_sentences2[i]), str(snli_test_gold_labels[i])) for i in range(snli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the snli-testset\".format(snli_test_num_samples))\n",
    "mnli_test_num_samples= len(mnli_test_gold_labels)\n",
    "mnli_test_samples = [(str(mnli_test_sentences1[i]), str(mnli_test_sentences2[i]), str(mnli_test_gold_labels[i])) for i in range(mnli_test_num_samples)]\n",
    "print(\"Using {} testing-samples from the multinli-testset\".format(mnli_test_num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use h5py to store the data. This helps to use less RAM for the training data in training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to include the word embeddings into the model is done after this keras-example:\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 0.0 #there is some extra testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for padding without using numpy, which breaks when building data arrays with 500000 or more embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_padding(sequences, maxlen=100):\n",
    "    for sequence in sequences:\n",
    "        while len(sequence) < maxlen:\n",
    "            sequence.append(0)\n",
    "        if len(sequence) > maxlen:\n",
    "            del sequence[maxlen:]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn string sequences into integer sequences, pad them to equal length, divide into training and validation data, store using the hdf5 binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91460 unique tokens.\n",
      "Saving word index...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7bcd9d63b321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving word index...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../data/word_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "str_sentences1 = [sentence for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "str_sentences2 = [sentence2 for sentence, sentence2, label in snli_samples + mnli_samples]\n",
    "labels = [label for sentence, sentence2, label in snli_samples + mnli_samples] \n",
    "\n",
    "snli_test_str_sentences1 = [sentence for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in snli_test_samples]\n",
    "snli_test_labels = [label for sentence, sentence2, label in snli_test_samples] \n",
    "\n",
    "mnli_test_str_sentences1 = [sentence for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_str_sentences2 = [sentence2 for sentence, sentence2, label in mnli_test_samples]\n",
    "mnli_test_labels = [label for sentence, sentence2, label in mnli_test_samples] \n",
    "\n",
    "numeric_labels = []\n",
    "for label in labels + snli_test_labels + mnli_test_labels:\n",
    "    if label == 'neutral':\n",
    "        numeric_labels.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        numeric_labels.append(1)\n",
    "    elif label == 'entailment':\n",
    "        numeric_labels.append(2)\n",
    "    else:\n",
    "        numeric_labels.append(3)\n",
    "        \n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(str_sentences1 + str_sentences2 + \n",
    "                       snli_test_str_sentences1 + snli_test_str_sentences2 +\n",
    "                      mnli_test_str_sentences1 + mnli_test_str_sentences2)\n",
    "sequences1 = tokenizer.texts_to_sequences(str_sentences1)\n",
    "sequences2 = tokenizer.texts_to_sequences(str_sentences2)\n",
    "snli_test_sequences1 = tokenizer.texts_to_sequences(snli_test_str_sentences1)\n",
    "snli_test_sequences2 = tokenizer.texts_to_sequences(snli_test_str_sentences2)\n",
    "mnli_test_sequences1 = tokenizer.texts_to_sequences(mnli_test_str_sentences1)\n",
    "mnli_test_sequences2 = tokenizer.texts_to_sequences(mnli_test_str_sentences2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Tokenizer...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Tokenizer...\")\n",
    "pickle.dump(tokenizer, open(\"../data/tokenizer.p\", \"wb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving word index...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving word index...\")\n",
    "pickle.dump(word_index, open(\"../data/word_index.p\", \"wb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word index...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word index...\")\n",
    "word_index = pickle.load(open(\"../data/word_index.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of sequences1: {}.\".format(np.shape(sequences1)))\n",
    "print(\"shape of sequences2: {}.\".format(np.shape(sequences2)))\n",
    "print(\"shape of snli_test_sequences1: {}.\".format(np.shape(snli_test_sequences1)))\n",
    "print(\"shape of snli_test_sequences2: {}.\".format(np.shape(snli_test_sequences2)))\n",
    "print(\"mhape of snli_test_sequences1: {}.\".format(np.shape(mnli_test_sequences1)))\n",
    "print(\"mhape of snli_test_sequences2: {}.\".format(np.shape(mnli_test_sequences2)))\n",
    "\n",
    "data1 = simple_padding(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = simple_padding(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data1 = simple_padding(snli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "snli_test_data2 = simple_padding(snli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data1 = simple_padding(mnli_test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "mnli_test_data2 = simple_padding(mnli_test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"shape of data1: {}.\".format(np.shape(data1)))\n",
    "print(\"shape of data2: {}.\".format(np.shape(data2)))\n",
    "print(\"shape of snli_test_data1: {}.\".format(np.shape(snli_test_data1)))\n",
    "print(\"shape of snli_test_data2: {}.\".format(np.shape(snli_test_data2)))\n",
    "print(\"shape of mnli_test_data1: {}.\".format(np.shape(mnli_test_data1)))\n",
    "print(\"shape of mnli_test_data2: {}.\".format(np.shape(mnli_test_data2)))\n",
    "\n",
    "training_data = np.asarray([data1, data2])\n",
    "snli_testing_data = np.asarray([snli_test_data1, snli_test_data2])\n",
    "mnli_testing_data = np.asarray([mnli_test_data1, mnli_test_data2])\n",
    "\n",
    "all_labels = to_categorical(np.asarray(numeric_labels))\n",
    "training_labels = all_labels[:len(labels)]\n",
    "snli_testing_labels = all_labels[len(labels):len(labels) + len(snli_test_labels)]\n",
    "mnli_testing_labels = all_labels[len(labels) + len(snli_test_labels):]\n",
    "\n",
    "##########CONTINUE HERE###############\n",
    "\n",
    "print('Shape of training_data tensor:', training_data.shape)\n",
    "print('Shape of training_labels tensor:', training_labels.shape)\n",
    "print('Shape of snli_testing_data tensor:', snli_testing_data.shape)\n",
    "print('Shape of snli_testing_labels tensor:', snli_testing_labels.shape)\n",
    "print('Shape of mnli_testing_data tensor:', mnli_testing_data.shape)\n",
    "print('Shape of mnli_testing_labels tensor:', mnli_testing_labels.shape)\n",
    "\n",
    "# not needed here, because training and testing data are already splitted\n",
    "# split the data into a training set and a validation set\n",
    "#indices = np.arange(data.shape[1])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[:,indices,:]\n",
    "#labels = labels[indices,:]\n",
    "#num_validation_samples = int(VALIDATION_SPLIT * data.shape[1])\n",
    "\n",
    "#x_train = data[:,:-num_validation_samples,:]\n",
    "#y_train = labels[:-num_validation_samples]\n",
    "#x_val = data[:,-num_validation_samples:,:]\n",
    "#y_val = labels[-num_validation_samples:]\n",
    "\n",
    "#print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "#print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "#print(\"Shape of x_val: {}\".format(np.shape(x_val)))\n",
    "#print(\"Shape of y_val: {}\".format(np.shape(y_val)))\n",
    "\n",
    "print(\"Storing training and test data to hdf5...\")\n",
    "\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "if not 'training_data' in f: x_train = f.create_dataset('training_data', data=training_data)\n",
    "if not 'training_labels' in f: y_train = f.create_dataset('training_labels', data=training_labels)\n",
    "if not 'snli_testing_data' in f: snli_x_val = f.create_dataset('snli_testing_data', data=snli_testing_data)\n",
    "if not 'snli_testing_labels' in f: snli_y_val = f.create_dataset('snli_testing_labels', data=snli_testing_labels)\n",
    "if not 'mnli_testing_data' in f: mnli_x_val = f.create_dataset('mnli_testing_data', data=mnli_testing_data)\n",
    "if not 'mnli_testing_labels' in f: mnli_y_val = f.create_dataset('mnli_testing_labels', data=mnli_testing_labels)\n",
    "    \n",
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))\n",
    "f.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an embedding matrix (the first layer of later models) and store it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "shape of embedding matrix: (91461, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=Reshape{3}...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "91461\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM)) #TODO: is this correct? In the example, the first dimension is\n",
    "# num_words, this, however, throws an error when populating the embedding matrix (because the word-indices start at 1, not at 0)\n",
    "print(\"shape of embedding matrix: {}\".format(np.shape(embedding_matrix)))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print(\"{}: {}\".format(i,word))\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = None\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = vectorspace[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #set trainable = True to enable training of the embeddings to the task at hand\n",
    "\n",
    "#save as a model for later use\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "\n",
    "embedding_model = Model(input=sentence_input, output=embedded_sentence)\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_model(embedding_model, '../data/embedding_layer.hdf5')\n",
    "print(embedding_layer.input_dim)\n",
    "print(embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "(None, 100)\n",
      "(None, 100, 300)\n",
      "(1, 91461, 300)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          27438300  \n",
      "=================================================================\n",
      "Total params: 27,438,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "embedding_model = load_model('../data/embedding_layer.hdf5')\n",
    "print('Done.')\n",
    "print(embedding_model.input_shape)\n",
    "print(embedding_model.output_shape)\n",
    "print(np.shape(embedding_model.get_weights()))\n",
    "print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most simple model for encoding the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#encode first sentence\n",
    "embedded_sentence1 = embedding_model(sentence1_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sentence1)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "encoded_sentence1 = MaxPooling1D(35)(x)\n",
    "\n",
    "#encode second sentence\n",
    "embedded_sentence2 = embedding_layer(sentence2_input)\n",
    "y = Conv1D(128, 5, activation='relu')(embedded_sentence2)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "y = MaxPooling1D(5)(y)\n",
    "y = Conv1D(128, 5, activation='relu')(y)\n",
    "encoded_sentence2 = MaxPooling1D(35)(y)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [encoded_sentence1, encoded_sentence2], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "model1 = Model(input=[sentence1_input, sentence2_input], output=preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 300)     2482200     input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 996, 128)      192128      embedding_1[3][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 996, 128)      192128      embedding_1[4][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_14 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_14 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_15 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_15 (MaxPooling1D)   (None, 1, 128)        0           convolution1d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 1, 256)        0           maxpooling1d_12[0][0]            \n",
      "                                                                   maxpooling1d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 256)           0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           65792       flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             1028        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,261,468\n",
      "Trainable params: 779,268\n",
      "Non-trainable params: 2,482,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same model but using a shared architecture for embedding both sentences, so that it will profit from all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "conv_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 22, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 5, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "=================================================================\n",
      "Total params: 27,652,188\n",
      "Trainable params: 213,888\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 1, 128)        27652188                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 256)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4)             1028                                         \n",
      "====================================================================================================\n",
      "Total params: 27,784,800\n",
      "Trainable params: 346,500\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_model.summary()\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for later: why doesn't convolution change the output size? Shouldnt it go down from to 100 to 98? -> solved, was because of bordermode = \"same\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of x_train: {}\".format(np.shape(f['training_data'])))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(f['training_labels'])))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(f['snli_testing_data'])))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(f['snli_testing_labels'])))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(f['mnli_testing_data'])))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(f['mnli_testing_labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(x_train, y_train, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_train[0,i:i+batch_size,:],x_train[1,i:i+batch_size,:]], y_train[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output\n",
    "\n",
    "def val_data_generator(x_test, y_test, num_batches, batch_size):\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = ([x_test[0,i:i+batch_size,:],x_test[1,i:i+batch_size,:]], y_test[i:i+batch_size])\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        yield gen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Batches per epoch training: 3677\n",
      "Batches per epoch validation: 39\n",
      "Training the model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7119efb1fb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m conv_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n\u001b[0m\u001b[1;32m     28\u001b[0m           \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "       \n",
    "batch_size = 256\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of batches until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Batches per epoch training: {}\".format(samples_per_epoch))\n",
    "print(\"Batches per epoch validation: {}\".format(nb_val_samples))\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "conv_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=10,\n",
    "          steps_per_epoch = samples_per_epoch,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"saving the model...\")\n",
    "conv_model.save_weights('../models/conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weights...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model weights...\")\n",
    "conv_model.load_weights('../models/conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the mnli data\n",
      "Score: 1.0824565573742515, Accuracy: 0.6094777960526315\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Evaluating on the mnli data\")\n",
    "batch_size = 256\n",
    "nb_val_samples = int(np.floor(np.shape(mnli_y_val)[0] / batch_size))\n",
    "\n",
    "score, acc = conv_model.evaluate_generator(val_data_generator(mnli_x_val, mnli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "                            steps = nb_val_samples)\n",
    "print(\"Score: {}, Accuracy: {}\".format(score, acc))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Evaluating on the mnli data\n",
      "Score: 0.914578778621478, Accuracy: 0.6699719551282052\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Evaluating on the mnli data\")\n",
    "batch_size = 256\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "score, acc = conv_model.evaluate_generator(val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "                            steps = nb_val_samples)\n",
    "print(\"Score: {}, Accuracy: {}\".format(score, acc))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Convolutional Model that also returns the encoded sentences as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(sentence_embedding)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "encoded_sentence = MaxPooling1D(5)(x)\n",
    "\n",
    "sentence_embedding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = sentence_embedding_model(sentence1_input)\n",
    "sentence2_embedding = sentence_embedding_model(sentence2_input)\n",
    "\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "flat = Flatten()(merged_vector)\n",
    "x = Dense(256, activation='relu')(flat)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(4,activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "alt_conv_model = Model(inputs=[sentence1_input, sentence2_input], outputs=[preds, sentence1_embedding, sentence2_embedding])\n",
    "alt_conv_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weights...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model weights...\")\n",
    "alt_conv_model.load_weights('../models/conv_model_snli+mnli.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "??LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:31: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, LSTM, Bidirectional, Dropout, TimeDistributed\n",
    "#from keras.layers.wrappers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#1. Embed\n",
    "sentence_embedding = embedding_model(sentence_input)\n",
    "\n",
    "#2. Encode\n",
    "encoded_sentence = Bidirectional(LSTM(32, activation='relu', return_sequences = True))(sentence_embedding)\n",
    "encoded_sentence = TimeDistributed(Dense(64, activation='relu'))(encoded_sentence)\n",
    "encoded_sentence = TimeDistributed(Dropout(0.2))(encoded_sentence)\n",
    "\n",
    "lstm_sentence_encoding_model = Model(inputs=sentence_input, outputs=encoded_sentence)\n",
    "\n",
    "#3. Attend\n",
    "\n",
    "\n",
    "sentence1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "sentence2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "sentence1_embedding = lstm_sentence_encoding_model(sentence1_input)\n",
    "sentence2_embedding = lstm_sentence_encoding_model(sentence2_input)\n",
    "\n",
    "\n",
    "#4. Predict\n",
    "#merge the encoded sentences (First: concatenation)\n",
    "merged_vector = merge(inputs = [sentence1_embedding, sentence2_embedding], mode='concat', concat_axis=-1)\n",
    "\n",
    "#predict the labels\n",
    "#flat = Flatten()(merged_vector)\n",
    "#merged_vector = Dropout(0.5)(merged_vector)\n",
    "x = Dense(64, activation='relu')(merged_vector)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(4, activation='softmax')(x)\n",
    "\n",
    "#compile the model\n",
    "clip_adam = Adam(clipnorm=1.)\n",
    "lstm_model = Model(inputs=[sentence1_input, sentence2_input], outputs=preds)\n",
    "lstm_model.compile(loss='mse',\n",
    "             optimizer=clip_adam,\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 300)\n",
      "(None, 100, 64)\n",
      "(None, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_model.output_shape)\n",
    "print(lstm_sentence_embedding_model.output_shape)\n",
    "print(lstm_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 100, 300)          27438300  \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 64)           85248     \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 100, 64)           4160      \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 100, 64)           0         \n",
      "=================================================================\n",
      "Total params: 27,527,708\n",
      "Trainable params: 89,408\n",
      "Non-trainable params: 27,438,300\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_18 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_19 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_11 (Model)                 (None, 100, 64)       27527708                                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 100, 128)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 100, 64)       8256                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 100, 4)        260                                          \n",
      "====================================================================================================\n",
      "Total params: 27,536,224\n",
      "Trainable params: 97,924\n",
      "Non-trainable params: 27,438,300\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_sentence_embedding_model.summary()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of x_train: (2, 941317, 100)\n",
      "Shape of y_train: (941317, 4)\n",
      "Shape of snli_x_val: (2, 10000, 100)\n",
      "Shape of snli_y_val: (10000, 4)\n",
      "Shape of mnli_x_val: (2, 9897, 100)\n",
      "Shape of mnli_y_val: (9897, 4)\n",
      "Training the Reccurent model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=7354, validation_data=<generator..., verbose=2, epochs=5, validation_steps=78)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected dense_18 to have 3 dimensions, but got array with shape (128, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-65f7833fd58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnli_x_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnli_y_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           verbose=2)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1876\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1615\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1616\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1301\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1302\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/fabian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    119\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected dense_18 to have 3 dimensions, but got array with shape (128, 4)"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "x_train = f['training_data']\n",
    "y_train = f['training_labels']\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of snli_x_val: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli_y_val: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli_x_val: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli_y_val: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "batch_size = 128\n",
    "samples_per_epoch = int(np.floor(np.shape(y_train)[0] / batch_size)) #num of batches until epoch is finished\n",
    "nb_val_samples = int(np.floor(np.shape(snli_y_val)[0] / batch_size))\n",
    "\n",
    "print(\"Training the Reccurent model...\")\n",
    "\n",
    "lstm_model.fit_generator(training_data_generator(x_train, y_train, num_batches=samples_per_epoch, batch_size= batch_size), \n",
    "          nb_epoch=5,\n",
    "          steps_per_epoch = samples_per_epoch,\n",
    "          validation_data=val_data_generator(snli_x_val, snli_y_val, num_batches=nb_val_samples, batch_size= batch_size),\n",
    "          nb_val_samples = nb_val_samples,\n",
    "          verbose=2)\n",
    "print(\"Done.\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I get nans as loss? -> exploding gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"saving gru model...\")\n",
    "lstm_model.save_weights('../data/lstm_model_weights.hdf5')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not too bad! Next up:\n",
    "- improving the convolution architecture (num filters, maxpooling size)\n",
    "- trying out lstms for encoding the sentences\n",
    "- testing with the actual repeval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A recursive network implementation in theano: https://github.com/ofirnachum/tree_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error Analysis\n",
    "\n",
    "Analyze which sentences are classified wrong and why:\n",
    "- print out target, prediction, sentence 1 and sentence 2 of wrongly classified samples (save to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word index...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word index...\")\n",
    "word_index = pickle.load(open(\"../data/word_index.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of snli samples: (2, 10000, 100)\n",
      "Shape of snli targets: (10000, 4)\n",
      "Shape of mnli samples: (2, 9897, 100)\n",
      "Shape of mnli samples: (9897, 4)\n",
      "Found 6030 correct and 3867 false classifications.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation data...\")\n",
    "f = h5py.File('../data/deep_training_data.hdf5', 'a')\n",
    "snli_x_val = f['snli_testing_data']\n",
    "snli_y_val = f['snli_testing_labels']\n",
    "mnli_x_val = f['mnli_testing_data']\n",
    "mnli_y_val = f['mnli_testing_labels']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Shape of snli samples: {}\".format(np.shape(snli_x_val)))\n",
    "print(\"Shape of snli targets: {}\".format(np.shape(snli_y_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_x_val)))\n",
    "print(\"Shape of mnli samples: {}\".format(np.shape(mnli_y_val)))\n",
    "\n",
    "wrong_classifications = []\n",
    "correct_classifications = []\n",
    "\n",
    "\n",
    "index_to_word = {v: k for k, v in word_index.items()} #maps from integer word indices to the words themselves\n",
    "label_to_word = {0: 'neutral', 1: 'contradiction', 2:'entailment', 3:'other'}\n",
    "\n",
    "num_samples = np.shape(mnli_x_val)[1]\n",
    "                 \n",
    "for i in range(num_samples):\n",
    "    sequence1 = np.reshape(mnli_x_val[0,i,:], (1,100))\n",
    "    sequence2 = np.reshape(mnli_x_val[1,i,:], (1,100))\n",
    "    y_pred = np.argmax(conv_model.predict([sequence1, sequence2]))\n",
    "    y_true = np.argmax(mnli_y_val[i])\n",
    "    #print(y_pred);print(y_true)\n",
    "    \n",
    "    sentence1 = \"\"\n",
    "    for index in sequence1[0]:\n",
    "        if index != 0: \n",
    "            sentence1 += index_to_word[index] + \" \"\n",
    "    sentence2 = \"\"\n",
    "    for index in sequence2[0]:\n",
    "        if index != 0: \n",
    "            sentence2 += index_to_word[index] + \" \"\n",
    "            \n",
    "    if y_pred != y_true:            \n",
    "        wrong_classifications.append((y_true, y_pred, sentence1, sentence2))\n",
    "    elif y_pred == y_true:\n",
    "        correct_classifications.append((y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "print(\"Found {} correct and {} false classifications.\".format(len(correct_classifications),len(wrong_classifications)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Done.\n",
      "Shape of snli samples: (2, 10000, 100)\n",
      "Shape of snli targets: (10000, 4)\n",
      "Shape of mnli samples: (2, 9897, 100)\n",
      "Shape of mnli samples: (9897, 4)\n",
      "Found 6030 correct and 3867 false classifications.\n"
     ]
    }
   ],
   "source": [
    "with open('../results/correct_classifications.txt', \"w\") as file:\n",
    "    for y_true, y_false, sentence1, sentence2 in correct_classifications:\n",
    "        file.write('----------------------\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_false], sentence1, sentence2))\n",
    "\n",
    "\n",
    "with open('../results/classification_errors.txt', \"w\") as file:\n",
    "    for y_true, y_false, sentence1, sentence2 in wrong_classifications:\n",
    "        file.write('----------------------\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_false], sentence1, sentence2))\n",
    "\n",
    "                 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How does a sentence need to change for a change in the output of the model?\n",
    "TODO: \n",
    "- write a function that turns one sentence into the right format and then computes the output of the model\n",
    "- test the different things:\n",
    "    - permutations\n",
    "    - negation\n",
    "    - exclude trigrams from the sentence\n",
    "- do this for 100 correctly classified and 100 wrongly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = pickle.load(open(\"../data/tokenizer.p\", \"rb\"))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_strings(string1, string2, tokenizer, model):\n",
    "    sequences = tokenizer.texts_to_sequences([string1, string2])\n",
    "    padded_sequences = simple_padding(sequences)\n",
    "    input_1 = np.reshape(np.array(padded_sequences[0]), (1,100))\n",
    "    input_2 = np.reshape(np.array(padded_sequences[1]), (1,100))\n",
    "    output_model = model.predict([input_1, input_2])\n",
    "    if len(output_model) == 1:\n",
    "        y_pred = np.argmax(output_model)\n",
    "        return y_pred\n",
    "    elif len(output_model) == 3:\n",
    "        y_pred = np.argmax(output_model[0])\n",
    "        return y_pred, output_model[1], output_model[2]\n",
    "    \n",
    "label_conv_dict = {\n",
    "    'neutral' : 0,\n",
    "    'contradiction' : 1,\n",
    "    'entailment' : 2,\n",
    "    'other' : 3,\n",
    "    0 : 'neutral',\n",
    "    1 : 'contradiction',\n",
    "    2 : 'entailment',\n",
    "    3 : 'other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_conv_dict[predict_strings(\"john not love mary\", \"john knows mary\", tokenizer, conv_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction\n",
      "(1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "#small test of the alternative conv model\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(\"john not love mary\", \"john knows mary\", tokenizer, alt_conv_model)\n",
    "print(label_conv_dict[y_true])\n",
    "print(sentence1_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0,\n",
       " \"'and i don't want to risk a fire fight with what appear to be horribly equal numbers ' \",\n",
       " \"i don't want to fight when we both have 1000 people \")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(correct_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, ngrams\n",
    "import re\n",
    "def exclude_trigrams(string):\n",
    "    toks = word_tokenize(string)\n",
    "    trigrams = ngrams(toks, 3)\n",
    "    corrupted_list = []\n",
    "    for tri in trigrams:\n",
    "        reduced_string = string\n",
    "        for i in range(0, 3):\n",
    "            regex = r\"\\b({})\\b\".format(tri[i])\n",
    "            reduced_string = re.sub(regex, '', reduced_string)\n",
    "        corrupted_list.append(reduced_string)\n",
    "    return corrupted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['   a test sentence, it does not really mean anything', 'this    test sentence, it does not really mean anything', 'this is    sentence, it does not really mean anything', 'this is just   , it does not really mean anything', 'this is just a  , it does not really mean anything', 'this is just a test ,  does not really mean anything', 'this is just a test sentence,   not really mean anything', 'this is just a test sentence,    really mean anything', 'this is just a test sentence, it    mean anything', 'this is just a test sentence, it does    anything', 'this is just a test sentence, it does not   ']\n"
     ]
    }
   ],
   "source": [
    "t = \"this is just a test sentence, it does not really mean anything\"\n",
    "print(exclude_trigrams(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'target': 1, 'pred': 1, 'sentence1': \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'sentence2': 'they always reach their potential and become successful ', 'corruptions': [(1, 1, False, \"   know thinking that 're going to have money and jobs and success and everything and then  then there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all    thinking that 're going to have money and jobs and success and everything and then  then there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they    that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you    they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know   're going to have money and jobs and success and everything and then  then there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking  're going to have money and jobs and success and everything and then  then there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're  to have money and jobs and success and everything and then  then there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they   have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're    money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going    and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no  and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to    jobs  success  everything  then they then there is no jobs  they end up homeless  not knowing anybody  no   it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have     success  everything  then they then there is no   they end up homeless  not knowing anybody  no   it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money    success  everything  then they then there is no   they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money      everything  then they then there is no   they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs    everything  then they then there is no jobs  they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs      then they then there is no jobs  they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success    then they then there is no jobs  they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success     they  there is no jobs  they end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money  jobs  success  everything     there is no jobs   end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money and jobs and success and everything and    there is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money and jobs and success and everything and     is no jobs and  end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and  they    no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then    jobs and they end up homeless and not knowing anybody and  money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and  and success and everything and then they then there    and they end up homeless and not knowing anybody and  money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money    success  everything  then they then there is    they end up homeless  not knowing anybody   money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money    success  everything  then  then there is no    end up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money  jobs  success  everything  then  then there is no jobs    up homeless  not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all  you know thinking that 're going to have money and jobs and success and everything and then  then there is no jobs and    homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they    and not knowing anybody and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is no jobs  they end    not knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is no jobs  they end up    knowing anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is no jobs  they end up homeless    anybody  no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and    and no money and it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is no jobs  they end up homeless  not    no money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is  jobs  they end up homeless  not knowing    money  it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have   jobs  success  everything  then they then there is  jobs  they end up homeless  not knowing anybody     it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have   jobs  success  everything  then they then there is  jobs  they end up homeless  not knowing anybody     it's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have   jobs  success  everything  then they then there is no jobs  they end up homeless  not knowing anybody  no   's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money  jobs  success  everything  then they then there is no jobs  they end up homeless  not knowing anybody  no money  's terrible \", 'they always reach their potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and 's  \", 'they always reach their potential and become successful '), (1, 0, True, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", '   their potential and become successful '), (1, 0, True, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they    potential and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always    and become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach    become successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their    successful '), (1, 1, False, \"all they you know thinking that they're going to have money and jobs and success and everything and then they then there is no jobs and they end up homeless and not knowing anybody and no money and it's terrible \", 'they always reach their potential    ')]}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2):\n",
    "    results = []  #a list of target, pred, sentence1, sentence2 with corruptions\n",
    "    #exlclude all possible trigrams\n",
    "    corrupted_sentence1 = exclude_trigrams(sentence1)\n",
    "    corrupted_sentence2 = exclude_trigrams(sentence2)\n",
    "    for corrupted_sentence in corrupted_sentence1:\n",
    "        corrupted_pred = predict_strings(corrupted_sentence, sentence2, tokenizer, conv_model)\n",
    "        changed = False\n",
    "        if corrupted_pred != y_pred:\n",
    "            changed = True\n",
    "        results.append((y_true, corrupted_pred, changed, corrupted_sentence, sentence2))\n",
    "    for corrupted_sentence in corrupted_sentence2:\n",
    "        corrupted_pred = predict_strings(sentence1, corrupted_sentence, tokenizer, conv_model)\n",
    "        changed = False\n",
    "        if corrupted_pred != y_pred:\n",
    "            changed = True\n",
    "        results.append((y_true, corrupted_pred, changed, sentence1, corrupted_sentence))\n",
    "    \n",
    "    return {'target' : y_true, 'pred' : y_pred, 'sentence1':sentence1, 'sentence2':sentence2,\n",
    "                         'corruptions' : results}\n",
    "NUM_SAMPLES = 100\n",
    "corruption_results = []\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    #take NUM_SAMPLES correct and NUM_SAMPLES wrong classifications\n",
    "    y_true, y_pred, sentence1, sentence2 = random.choice(correct_classifications)\n",
    "    corruption_results.append(apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "    #the same for wrong classifications\n",
    "    y_true, y_pred, sentence1, sentence2 = random.choice(wrong_classifications)\n",
    "    corruption_results.append(apply_corruption_to_sample(y_true, y_pred, sentence1, sentence2))\n",
    "    \n",
    "print(len(corruption_results))\n",
    "print(corruption_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print results to file\n",
    "with open('../results/corrupted_sentences.txt', \"w\") as file:\n",
    "    for sample_results in corruption_results:\n",
    "        y_true = sample_results['target']\n",
    "        y_pred = sample_results['pred']\n",
    "        sentence1 = sample_results['sentence1']\n",
    "        sentence2 = sample_results['sentence2']\n",
    "        file.write('########################################\\nTARGET: {}, PRED: {}\\nSENTENCE1: {}\\nSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[y_pred], sentence1, sentence2))\n",
    "        for (y_true, corrupted_pred, changed, corrupted_sentence, sentence2) in sample_results['corruptions']:\n",
    "            file.write('\\t--------\\n\\tTARGET: {}, PRED: {}, CHANGED = {}\\n\\tSENTENCE1: {}\\n\\tSENTENCE2: {}\\n'.format(label_to_word[y_true], label_to_word[corrupted_pred], changed, corrupted_sentence, sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring spaCy dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_depency(string):\n",
    "    doc = nlp(string)\n",
    "    dep_labels = []\n",
    "    for token in doc:\n",
    "        #print(token, token.dep_, token.pos_)\n",
    "        dep_labels.append((token, token.dep_, token.pos_))\n",
    "    return dep_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_negation(string):\n",
    "    parsed_string = parse_depency(string)\n",
    "    result = ''\n",
    "    for tok, dep, pos in parsed_string:\n",
    "        result += tok.text + ' '\n",
    "        if pos == 'VERB':\n",
    "            result += 'not '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(i, 'nsubj', 'PRON'),\n",
       " (am, 'ROOT', 'VERB'),\n",
       " (fabian, 'acomp', 'ADJ'),\n",
       " (and, 'cc', 'CCONJ'),\n",
       " (i, 'nsubj', 'PRON'),\n",
       " (like, 'prep', 'VERB'),\n",
       " (butter, 'pobj', 'NOUN')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_depency('i am fabian and i like butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am not fabian and i like not butter '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_negation('i am fabian and i like butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 'i am fabian and i like butter' with 'i am fabian and i like butter'\n",
      "predicted target:  entailment\n",
      "4.05615794286e-08\n",
      "\n",
      "Comparing 'i am fabian and i like butter' with 'i am fabian and i not like butter'\n",
      "predicted target:  contradiction\n",
      "0.198232791941\n",
      "\n",
      "Comparing 'i am fabian and i like butter' with 'some test sentence for comparison'\n",
      "predicted target:  entailment\n",
      "0.49842118078\n"
     ]
    }
   ],
   "source": [
    "import scipy as scp\n",
    "\n",
    "sentence1 = \"i am fabian and i like butter\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence1, sentence1, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence1))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n",
    "print()\n",
    "\n",
    "sentence2 = \"i am fabian and i not like butter\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence2, sentence1, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence2))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n",
    "print()\n",
    "\n",
    "sentence3 = \"some test sentence for comparison\"\n",
    "y_true, sentence1_encoding, sentence2_encoding = predict_strings(sentence1, sentence3, tokenizer, alt_conv_model)\n",
    "print(\"Comparing '{}' with '{}'\".format(sentence1,sentence3))\n",
    "print('predicted target: ',label_conv_dict[y_true])\n",
    "print(scp.spatial.distance.cosine(sentence1_encoding, sentence2_encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the model does what it should. As the sentence is negated, the predicted entailment changes, and the distance of the sentence encodings also increases but is comparably small regarding the distance to a completely different sentence encoding. \n",
    "\n",
    "Even more interesting to me is that in the third example, the model votes for entailment although the difference in embedding space is much higher than in example three. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
