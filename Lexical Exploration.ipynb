{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of explicitly encoding Lexical Patterns into Neural Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import composition_learning\n",
    "from gensim.models import KeyedVectors as embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst erstmal Zielnomen, Nomen, Adjektive aus tsv-File laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "path_to_adj_noun_file = \"../data/amod.train.filtered\"\n",
    "gnoun_noun_adj_list = [] #a list of lists, each sublist containing a goal-noun, noun, adjective triple\n",
    "goal_noun_list = [] #only the goal nouns, needed for functions from composition_learning.py.\n",
    "with open(path_to_adj_noun_file, \"r\") as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        gnoun_noun_adj_list.append(row)\n",
    "        goal_noun_list.append(row[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach Word Embeddings aus Modell laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_word_embeddings = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "vector_space = embedding_model.load_word2vec_format(path_to_word_embeddings, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten in das richtige Format für das Keras-Modell bringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, training_labels, not_in_list = composition_learning.construct_data_and_labels(gnoun_noun_adj_list,\n",
    "                                                                                             vector_space,\n",
    "                                                                                             goal_noun_list,\n",
    "                                                                                             verbosity = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (16198, 2, 300)\n",
      "Shape of training labels: (16198, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data: {}\".format(np.shape(training_data)))\n",
    "print(\"Shape of training labels: {}\".format(np.shape(training_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = h5py.File('../data/lexical_training_data.hdf5', 'w')\n",
    "f.create_dataset('training_data', data=training_data)\n",
    "f.create_dataset('training_labels', data=training_labels)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??g.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = h5py.File('../data/lexical_training_data.hdf5', 'r')\n",
    "training_data = g[\"training_data\"]\n",
    "training_labels = g[\"training_labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16198, 2, 300)\n",
      "(16198, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(training_data))\n",
    "print(np.shape(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_data_generator(training_data, training_labels, batch_size):\n",
    "    num_batches = int(np.floor(np.shape(training_data)[0] / batch_size))\n",
    "    i=0\n",
    "    while i < num_batches*batch_size:\n",
    "        gen_output = (training_data[i:i+batch_size], training_labels[i:i+batch_size])\n",
    "        #gen_output = ([training_data[i:i+batch_size,0,:],training_data[i:i+batch_size,1,:]], training_labels[i:i+batch_size])\n",
    "        #print(i, np.shape(gen_output))\n",
    "        i += batch_size\n",
    "        if i >= (num_batches*batch_size): i = 0\n",
    "        #print(i)\n",
    "        yield gen_output\n",
    "#TODO: nimmt insgesamt zu wenige samples pro epoche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = training_data_generator(t_data, t_labels, batch_size=64)\n",
    "print(np.shape(next(gen)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell trainieren. tensor_model bezeichnet das modell, indem adjektiv mit Tensor zusammen zu einer Matrix multipliziert wird, die dann mit Nomen-vektor multipliziert wird um wieder einen Vektor zu erhalten.\n",
    "weighted_model steht für ein additives Modell mit Gewichtigunsmatrizen für je Adjetiv und Nomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(gnoun_noun_adj_list))\n",
    "print(len(not_in_list))\n",
    "print(np.shape(training_data), np.shape(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainiere NN mit mode tensor_mult_identity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/work/ag_sc/repeval2017/repeval2017/composition_learning.py:229: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=63.0, verbose=2, epochs=10)`\n",
      "  model.fit_generator(data_generator,steps_per_epoch=steps_per_epoch, verbose=verbosity, nb_epoch=nb_epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8s - loss: -3.6268e-04 - acc: 0.0110\n",
      "Epoch 2/10\n",
      "8s - loss: -9.6484e-04 - acc: 0.0209\n",
      "Epoch 3/10\n",
      "8s - loss: -1.0975e-03 - acc: 0.0263\n",
      "Epoch 4/10\n",
      "8s - loss: -1.1757e-03 - acc: 0.0305\n",
      "Epoch 5/10\n",
      "8s - loss: -1.2301e-03 - acc: 0.0339\n",
      "Epoch 6/10\n",
      "8s - loss: -1.2711e-03 - acc: 0.0367\n",
      "Epoch 7/10\n",
      "8s - loss: -1.3038e-03 - acc: 0.0393\n",
      "Epoch 8/10\n",
      "8s - loss: -1.3306e-03 - acc: 0.0418\n",
      "Epoch 9/10\n",
      "8s - loss: -1.3534e-03 - acc: 0.0442\n",
      "Epoch 10/10\n",
      "8s - loss: -1.3730e-03 - acc: 0.0462\n"
     ]
    }
   ],
   "source": [
    "tensor_model = composition_learning.train_model(training_data_generator(training_data, training_labels, batch_size=batch_size), \n",
    "                                                steps_per_epoch = np.floor(np.shape(training_data)[0] / batch_size),\n",
    "                                                composition_mode = 'tensor_mult_identity', verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2, 300)            0         \n",
      "_________________________________________________________________\n",
      "magic_operation_2 (MagicOper (None, 300)               27000000  \n",
      "=================================================================\n",
      "Total params: 27,000,000\n",
      "Trainable params: 27,000,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tensor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainiere NN mit mode weighted_adj_and_noun_add_identity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/work/ag_sc/repeval2017/repeval2017/composition_learning.py:229: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=63, verbose=2, epochs=100)`\n",
      "  model.fit_generator(data_generator,steps_per_epoch=steps_per_epoch, verbose=verbosity, nb_epoch=nb_epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "0s - loss: -1.2208e-03 - acc: 0.0355\n",
      "Epoch 2/100\n",
      "0s - loss: -1.3556e-03 - acc: 0.0435\n",
      "Epoch 3/100\n",
      "0s - loss: -1.4057e-03 - acc: 0.0497\n",
      "Epoch 4/100\n",
      "0s - loss: -1.4372e-03 - acc: 0.0538\n",
      "Epoch 5/100\n",
      "0s - loss: -1.4603e-03 - acc: 0.0567\n",
      "Epoch 6/100\n",
      "0s - loss: -1.4782e-03 - acc: 0.0577\n",
      "Epoch 7/100\n",
      "0s - loss: -1.4927e-03 - acc: 0.0584\n",
      "Epoch 8/100\n",
      "0s - loss: -1.5046e-03 - acc: 0.0601\n",
      "Epoch 9/100\n",
      "0s - loss: -1.5147e-03 - acc: 0.0616\n",
      "Epoch 10/100\n",
      "0s - loss: -1.5233e-03 - acc: 0.0621\n",
      "Epoch 11/100\n",
      "0s - loss: -1.5307e-03 - acc: 0.0626\n",
      "Epoch 12/100\n",
      "0s - loss: -1.5372e-03 - acc: 0.0634\n",
      "Epoch 13/100\n",
      "0s - loss: -1.5428e-03 - acc: 0.0641\n",
      "Epoch 14/100\n",
      "0s - loss: -1.5478e-03 - acc: 0.0644\n",
      "Epoch 15/100\n",
      "0s - loss: -1.5522e-03 - acc: 0.0650\n",
      "Epoch 16/100\n",
      "0s - loss: -1.5561e-03 - acc: 0.0654\n",
      "Epoch 17/100\n",
      "0s - loss: -1.5596e-03 - acc: 0.0654\n",
      "Epoch 18/100\n",
      "0s - loss: -1.5628e-03 - acc: 0.0655\n",
      "Epoch 19/100\n",
      "0s - loss: -1.5656e-03 - acc: 0.0660\n",
      "Epoch 20/100\n",
      "0s - loss: -1.5681e-03 - acc: 0.0663\n",
      "Epoch 21/100\n",
      "0s - loss: -1.5704e-03 - acc: 0.0667\n",
      "Epoch 22/100\n",
      "0s - loss: -1.5725e-03 - acc: 0.0667\n",
      "Epoch 23/100\n",
      "0s - loss: -1.5744e-03 - acc: 0.0670\n",
      "Epoch 24/100\n",
      "0s - loss: -1.5761e-03 - acc: 0.0670\n",
      "Epoch 25/100\n",
      "0s - loss: -1.5775e-03 - acc: 0.0677\n",
      "Epoch 26/100\n",
      "0s - loss: -1.5771e-03 - acc: 0.0658\n",
      "Epoch 27/100\n",
      "0s - loss: -1.5754e-03 - acc: 0.0677\n",
      "Epoch 28/100\n",
      "0s - loss: -1.5795e-03 - acc: 0.0671\n",
      "Epoch 29/100\n",
      "0s - loss: -1.5822e-03 - acc: 0.0684\n",
      "Epoch 30/100\n",
      "0s - loss: -1.5836e-03 - acc: 0.0681\n",
      "Epoch 31/100\n",
      "0s - loss: -1.5847e-03 - acc: 0.0688\n",
      "Epoch 32/100\n",
      "0s - loss: -1.5855e-03 - acc: 0.0676\n",
      "Epoch 33/100\n",
      "0s - loss: -1.5860e-03 - acc: 0.0684\n",
      "Epoch 34/100\n",
      "0s - loss: -1.5861e-03 - acc: 0.0678\n",
      "Epoch 35/100\n",
      "0s - loss: -1.5860e-03 - acc: 0.0681\n",
      "Epoch 36/100\n",
      "0s - loss: -1.5869e-03 - acc: 0.0676\n",
      "Epoch 37/100\n",
      "0s - loss: -1.5882e-03 - acc: 0.0683\n",
      "Epoch 38/100\n",
      "0s - loss: -1.5893e-03 - acc: 0.0680\n",
      "Epoch 39/100\n",
      "0s - loss: -1.5900e-03 - acc: 0.0690\n",
      "Epoch 40/100\n",
      "0s - loss: -1.5905e-03 - acc: 0.0680\n",
      "Epoch 41/100\n",
      "0s - loss: -1.5908e-03 - acc: 0.0689\n",
      "Epoch 42/100\n",
      "0s - loss: -1.5911e-03 - acc: 0.0682\n",
      "Epoch 43/100\n",
      "0s - loss: -1.5913e-03 - acc: 0.0687\n",
      "Epoch 44/100\n",
      "0s - loss: -1.5917e-03 - acc: 0.0678\n",
      "Epoch 45/100\n",
      "0s - loss: -1.5922e-03 - acc: 0.0686\n",
      "Epoch 46/100\n",
      "0s - loss: -1.5928e-03 - acc: 0.0679\n",
      "Epoch 47/100\n",
      "0s - loss: -1.5933e-03 - acc: 0.0686\n",
      "Epoch 48/100\n",
      "0s - loss: -1.5937e-03 - acc: 0.0680\n",
      "Epoch 49/100\n",
      "0s - loss: -1.5940e-03 - acc: 0.0689\n",
      "Epoch 50/100\n",
      "0s - loss: -1.5942e-03 - acc: 0.0685\n",
      "Epoch 51/100\n",
      "0s - loss: -1.5944e-03 - acc: 0.0688\n",
      "Epoch 52/100\n",
      "0s - loss: -1.5947e-03 - acc: 0.0680\n",
      "Epoch 53/100\n",
      "0s - loss: -1.5950e-03 - acc: 0.0688\n",
      "Epoch 54/100\n",
      "0s - loss: -1.5953e-03 - acc: 0.0685\n",
      "Epoch 55/100\n",
      "0s - loss: -1.5956e-03 - acc: 0.0692\n",
      "Epoch 56/100\n",
      "0s - loss: -1.5959e-03 - acc: 0.0685\n",
      "Epoch 57/100\n",
      "0s - loss: -1.5961e-03 - acc: 0.0694\n",
      "Epoch 58/100\n",
      "0s - loss: -1.5963e-03 - acc: 0.0680\n",
      "Epoch 59/100\n",
      "0s - loss: -1.5966e-03 - acc: 0.0695\n",
      "Epoch 60/100\n",
      "0s - loss: -1.5967e-03 - acc: 0.0679\n",
      "Epoch 61/100\n",
      "0s - loss: -1.5969e-03 - acc: 0.0698\n",
      "Epoch 62/100\n",
      "0s - loss: -1.5971e-03 - acc: 0.0681\n",
      "Epoch 63/100\n",
      "0s - loss: -1.5973e-03 - acc: 0.0698\n",
      "Epoch 64/100\n",
      "0s - loss: -1.5975e-03 - acc: 0.0681\n",
      "Epoch 65/100\n",
      "0s - loss: -1.5977e-03 - acc: 0.0696\n",
      "Epoch 66/100\n",
      "0s - loss: -1.5979e-03 - acc: 0.0681\n",
      "Epoch 67/100\n",
      "0s - loss: -1.5981e-03 - acc: 0.0696\n",
      "Epoch 68/100\n",
      "0s - loss: -1.5982e-03 - acc: 0.0681\n",
      "Epoch 69/100\n",
      "0s - loss: -1.5984e-03 - acc: 0.0696\n",
      "Epoch 70/100\n",
      "0s - loss: -1.5985e-03 - acc: 0.0683\n",
      "Epoch 71/100\n",
      "0s - loss: -1.5987e-03 - acc: 0.0699\n",
      "Epoch 72/100\n",
      "0s - loss: -1.5988e-03 - acc: 0.0682\n",
      "Epoch 73/100\n",
      "0s - loss: -1.5990e-03 - acc: 0.0698\n",
      "Epoch 74/100\n",
      "0s - loss: -1.5991e-03 - acc: 0.0683\n",
      "Epoch 75/100\n",
      "0s - loss: -1.5993e-03 - acc: 0.0696\n",
      "Epoch 76/100\n",
      "0s - loss: -1.5994e-03 - acc: 0.0686\n",
      "Epoch 77/100\n",
      "0s - loss: -1.5995e-03 - acc: 0.0698\n",
      "Epoch 78/100\n",
      "0s - loss: -1.5997e-03 - acc: 0.0686\n",
      "Epoch 79/100\n",
      "0s - loss: -1.5998e-03 - acc: 0.0698\n",
      "Epoch 80/100\n",
      "0s - loss: -1.5999e-03 - acc: 0.0688\n",
      "Epoch 81/100\n",
      "0s - loss: -1.6000e-03 - acc: 0.0696\n",
      "Epoch 82/100\n",
      "0s - loss: -1.6001e-03 - acc: 0.0689\n",
      "Epoch 83/100\n",
      "0s - loss: -1.6002e-03 - acc: 0.0695\n",
      "Epoch 84/100\n",
      "0s - loss: -1.6004e-03 - acc: 0.0689\n",
      "Epoch 85/100\n",
      "0s - loss: -1.6005e-03 - acc: 0.0695\n",
      "Epoch 86/100\n",
      "0s - loss: -1.6006e-03 - acc: 0.0692\n",
      "Epoch 87/100\n",
      "0s - loss: -1.6007e-03 - acc: 0.0693\n",
      "Epoch 88/100\n",
      "0s - loss: -1.6008e-03 - acc: 0.0691\n",
      "Epoch 89/100\n",
      "0s - loss: -1.6009e-03 - acc: 0.0693\n",
      "Epoch 90/100\n",
      "0s - loss: -1.6010e-03 - acc: 0.0693\n",
      "Epoch 91/100\n",
      "0s - loss: -1.6011e-03 - acc: 0.0696\n",
      "Epoch 92/100\n",
      "0s - loss: -1.6012e-03 - acc: 0.0694\n",
      "Epoch 93/100\n",
      "0s - loss: -1.6012e-03 - acc: 0.0695\n",
      "Epoch 94/100\n",
      "0s - loss: -1.6014e-03 - acc: 0.0696\n",
      "Epoch 95/100\n",
      "0s - loss: -1.6014e-03 - acc: 0.0696\n",
      "Epoch 96/100\n",
      "0s - loss: -1.6016e-03 - acc: 0.0698\n",
      "Epoch 97/100\n",
      "0s - loss: -1.6016e-03 - acc: 0.0696\n",
      "Epoch 98/100\n",
      "0s - loss: -1.6017e-03 - acc: 0.0698\n",
      "Epoch 99/100\n",
      "0s - loss: -1.6018e-03 - acc: 0.0696\n",
      "Epoch 100/100\n",
      "0s - loss: -1.6019e-03 - acc: 0.0701\n"
     ]
    }
   ],
   "source": [
    "weighted_model = composition_learning.train_model(training_data_generator(training_data, training_labels, batch_size=batch_size), \n",
    "                                                steps_per_epoch = int(np.floor(np.shape(training_data)[0] / batch_size)),\n",
    "                                                composition_mode = 'weighted_adj_and_noun_add_identity', nb_epoch=100, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??composition_learning.train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_model.name = \"weighted_model_new\"\n",
    "tensor_model.name = \"tensor_model_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelle abspeichern für später(funktioniert irgendwie noch nicht, wahrscheinlich wegen eigener layerdefinition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_model.save_weights(\"../models/weighted_model_weights_new\", overwrite=True)\n",
    "tensor_model.save_weights(\"../models/tensor_model_weights_new\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd ../repeval2017/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelle wieder laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 10)\n",
      "<class 'theano.sandbox.cuda.var.CudaNdarraySharedVariable'>\n"
     ]
    }
   ],
   "source": [
    "init = composition_learning.init_3d_identity((10,10,10))\n",
    "print(type(init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_model = composition_learning.create_model(composition_mode = 'weighted_adj_and_noun_add_identity')\n",
    "weighted_model.load_weights(\"../models/weighted_model_weights\")\n",
    "tensor_model = composition_learning.create_model(composition_mode = 'tensor_mult_identity')\n",
    "#print(tensor_model.output_shape)\n",
    "\n",
    "#tensor_model.summary()\n",
    "tensor_model.load_weights(\"../models/tensor_model_weights\")\n",
    "\n",
    "weighted_model.name = \"weighted_model\"\n",
    "tensor_model.name = \"tensor_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um mit den Modellen Vektoren zu berechnen, tut man folgendes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goal_noun, noun, adj = gnoun_noun_adj_list[0]\n",
    "\n",
    "\n",
    "goal_noun_tensor_vector = tensor_model.predict(np.asarray([[vectorspace[adj], vectorspace[noun]]]))[0, 0]\n",
    "goal_noun_weighted_add_vector = weighted_model.predict(np.asarray([[vectorspace[adj], vectorspace[noun]]]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lade Datensatz von hier: http://jair.org/media/3640/live-3640-6413-jair.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd ../data/\n",
    "!wget \"http://jair.org/media/3640/live-3640-6413-jair.txt\"\n",
    "%ls\n",
    "%cd ../repeval2017/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluierungsdatensatz einlesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "path_to_eval_file = \"../data/live-3640-6413-jair.txt\"\n",
    "data = []\n",
    "with open(path_to_eval_file, \"r\") as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='|')\n",
    "    i = 0    \n",
    "    for row in tsv_reader:\n",
    "        if row[0][0] != '#':\n",
    "            data.append([])\n",
    "            for elem in row:\n",
    "                data[i].append(elem.strip())\n",
    "            i+=1\n",
    "\n",
    "training_data = data[0:680] #Split nach Angabe von Turney\n",
    "test_data = data[680:]\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??complete_vector_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sys\n",
    "\n",
    "def compute_distances_to_candidates(model, vector_space, sample, complete_vector_space=False, topn=150):\n",
    "    #print(\"Processing {}\".format(sample))\n",
    "    distances = []\n",
    "    not_in_vector_space = []\n",
    "    \n",
    "    phrase = sample[0]\n",
    "    adj,noun = phrase.split()\n",
    "    # berechne phrasenvektor mit modell\n",
    "    #print(\"Complete_vector_space = {}\".format(complete_vector_space))\n",
    "    try:      \n",
    "        if 'weighted_model' in model.name: phrase_vec = model.predict(np.asarray([[vector_space[adj], vector_space[noun]]]))[0]\n",
    "        elif 'tensor_model' in model.name: phrase_vec = model.predict(np.asarray([[vector_space[adj], vector_space[noun]]]))[0,0]\n",
    "    except:\n",
    "        not_in_vector_space.append([adj,noun])\n",
    "        return distances\n",
    "    \n",
    "    if complete_vector_space:\n",
    "        #print(np.shape(phrase_vec))\n",
    "        distances = vector_space.most_similar(positive=[phrase_vec], topn=topn)\n",
    "    elif not complete_vector_space:\n",
    "        for i in range(1,len(sample)):\n",
    "            # berechne vektoren für alle targets per look up\n",
    "            try:\n",
    "                target_vec = vector_space[sample[i]]\n",
    "                distance = cosine(phrase_vec, target_vec)\n",
    "                distances.append((sample[i], distance))\n",
    "            except KeyError as e:\n",
    "                distances.append((sample[i], np.infty))#wörter, die nicht im vektorraum enthalten sind, werden als unendlich weit weg behandelt\n",
    "                not_in_vector_space.append(sample[i])\n",
    "                #print(\"{} is not in vectorspace\".format(sample[i]))\n",
    "            \n",
    "    #sortiere nach distanz\n",
    "    sorted_distances = sorted(distances, key=lambda x:x[1], reverse=False)\n",
    "    #print(sorted_distances)\n",
    "    return sorted_distances\n",
    "    \n",
    "def eval_model(model, vector_space, test_data, mode='average_rank', k=1, topn=100):\n",
    "    if mode == 'average_rank':\n",
    "        ranks = []\n",
    "        empty_list_counter = 0\n",
    "        for sample in test_data:            \n",
    "            distances = compute_distances_to_candidates(model, vector_space, sample)\n",
    "            if distances:\n",
    "                for i in range(0, len(distances)):\n",
    "                    if distances[i][0] == sample[1]: #find rank of gold label\n",
    "                        #print(distances)\n",
    "                        ranks.append(i)\n",
    "            else:\n",
    "                empty_list_counter += 1\n",
    "        print(\"Could not compute rank in {} of {} cases due to empty distance lists.\\n(Probably due to words not appearing in the embedding space.)\".format(empty_list_counter, len(test_data)))\n",
    "        return np.mean(ranks)\n",
    "    elif mode == 'average_rank_complete_vectorspace':\n",
    "        ranks = []\n",
    "        for sample in test_data:\n",
    "            distances = compute_distances_to_candidates(model, vector_space, sample, complete_vector_space=True, topn=topn)\n",
    "            if distances:\n",
    "                for i in range(0, len(distances)):\n",
    "                    if distances[i][0] == sample[1]: #find rank of gold label\n",
    "                        #print(distances)\n",
    "                        ranks.append(i)\n",
    "                    else:\n",
    "                        ranks.append(topn + 1)\n",
    "            else:\n",
    "                empty_list_counter += 1\n",
    "        print(\"Could not compute rank in {} of {} cases due to empty distance lists.\\n(Probably due to words not appearing in the embedding space.)\".format(empty_list_counter, len(test_data)))\n",
    "        return np.mean(ranks)\n",
    "    elif mode == 'accuracy':\n",
    "        correct_counter = 0.0\n",
    "        empty_list_counter = 0\n",
    "        for sample in test_data:\n",
    "            distances = compute_distances_to_candidates(model, vector_space, sample)\n",
    "            if distances: \n",
    "                if distances[0][0] == sample[1]: #gold label is rank 1\n",
    "                    correct_counter += 1\n",
    "                else:\n",
    "                    correct_counter += 5/7\n",
    "            else:\n",
    "                empty_list_counter += 1\n",
    "                \n",
    "        print(\"Could not compute rank in {} of {} cases due to empty distance lists.(Probably due to words not appearing in the embedding space.)\".format(empty_list_counter, len(test_data)))\n",
    "        return (correct_counter / (len(test_data) - empty_list_counter))\n",
    "    elif mode == 'precision':\n",
    "        correct_counter = 0\n",
    "        empty_list_counter = 0\n",
    "        for sample in test_data:\n",
    "            distances = compute_distances_to_candidates(model, vector_space, sample)\n",
    "            if distances:\n",
    "                k_nearest = [target for target, distance in distances]\n",
    "                if sample[1] in k_nearest[:k]:\n",
    "                    correct_counter += 1\n",
    "            else:\n",
    "                empty_list_counter += 1\n",
    "        return (correct_counter / (len(test_data) - empty_list_counter))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Could not compute rank in 18 of 1500 cases due to empty distance lists.\n",
      "(Probably due to words not appearing in the embedding space.)\n",
      "WEIGHTED_MODEL_NEW: Average rank of the gold label: 2.6214574898785425\n",
      "--------------------------\n",
      "Could not compute rank in 18 of 1500 cases due to empty distance lists.(Probably due to words not appearing in the embedding space.)\n",
      "WEIGHTED_MODEL_NEW: Accuracy of the model: 0.7337574705995608\n",
      "--------------------------\n",
      "WEIGHTED_MODEL_NEW: Precision_at_1 of the model: 0.06815114709851552\n",
      "--------------------------\n",
      "WEIGHTED_MODEL_NEW: Precision_at_2 of the model: 0.3083670715249663\n",
      "--------------------------\n",
      "WEIGHTED_MODEL_NEW: Precision_at_3 of the model: 0.5654520917678812\n",
      "--------------------------\n",
      "Could not compute rank in 18 of 1500 cases due to empty distance lists.\n",
      "(Probably due to words not appearing in the embedding space.)\n",
      "TENSOR_MODEL_NEW: Average rank of the gold label: 2.591093117408907\n",
      "--------------------------\n",
      "Could not compute rank in 18 of 1500 cases due to empty distance lists.(Probably due to words not appearing in the embedding space.)\n",
      "TENSOR_MODEL_NEW: Accuracy of the model: 0.756699440909955\n",
      "--------------------------\n",
      "TENSOR_MODEL_NEW: Precision_at_1 of the model: 0.06815114709851552\n",
      "--------------------------\n",
      "TENSOR_MODEL_NEW: Precision_at_2 of the model: 0.3083670715249663\n",
      "--------------------------\n",
      "TENSOR_MODEL_NEW: Precision_at_3 of the model: 0.5654520917678812\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------\") \n",
    "print(\"{}: Average rank of the gold label: {}\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='average_rank')))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Accuracy of the model: {}\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='accuracy')))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Precision_at_1 of the model: {}\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision')))\n",
    "print(\"--------------------------\") \n",
    "print(\"{}: Precision_at_2 of the model: {}\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision', k=2)))\n",
    "print(\"--------------------------\") \n",
    "print(\"{}: Precision_at_3 of the model: {}\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision', k=3)))\n",
    "print(\"--------------------------\") \n",
    "print(\"{}: Average rank of the gold label: {}\".format(tensor_model.name.upper(), eval_model(tensor_model, vector_space, test_data, mode='average_rank')))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Accuracy of the model: {}\".format(tensor_model.name.upper(), eval_model(tensor_model, vector_space, test_data, mode='accuracy')))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Precision_at_1 of the model: {}\".format(tensor_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision')))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Precision_at_2 of the model: {}\".format(tensor_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision', k=2)))\n",
    "print(\"--------------------------\")\n",
    "print(\"{}: Precision_at_3 of the model: {}\".format(tensor_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='precision', k=3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{}: Average rank of the gold label: {} (whole vectorspace!)\".format(weighted_model.name.upper(), eval_model(weighted_model, vector_space, test_data, mode='average_rank_complete_vectorspace')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7142857142857144\n"
     ]
    }
   ],
   "source": [
    "i = 1 \n",
    "i += 5/7\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
